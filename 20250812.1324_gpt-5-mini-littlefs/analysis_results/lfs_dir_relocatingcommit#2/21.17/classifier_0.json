{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 21.17 is: Use of the string handling functions from <string.h> shall not result in accesses beyond the bounds of the objects referenced by their pointer parameters.\nThis rule strictly applies to the Standard Library functions in <string.h> such as memcpy, memmove, memset, memcmp, strcpy, strncpy, strcat, strncat, strlen, and strspn.\n</misra_rule>\n<response format>\nProvide your answer with these fields.\n- is_rule_violated: whether the rule is violated or not (true/false),\n- scenario: a specific violation scenario.\n</response format>\n<target_code>\nHere is the code of lfs_dir_relocatingcommit:\n```c\nstatic int lfs_dir_relocatingcommit(lfs_t *lfs, lfs_mdir_t *dir,\n        const lfs_block_t pair[2],\n        const struct lfs_mattr *attrs, int attrcount,\n        lfs_mdir_t *pdir) {\n    int state = 0;\n\n    // calculate changes to the directory\n    bool hasdelete = false;\n    for (int i = 0; i < attrcount; i++) {\n        if (lfs_tag_type3(attrs[i].tag) == LFS_TYPE_CREATE) {\n            dir->count += 1;\n        } else if (lfs_tag_type3(attrs[i].tag) == LFS_TYPE_DELETE) {\n            LFS_ASSERT(dir->count > 0);\n            dir->count -= 1;\n            hasdelete = true;\n        } else if (lfs_tag_type1(attrs[i].tag) == LFS_TYPE_TAIL) {\n            dir->tail[0] = ((lfs_block_t*)attrs[i].buffer)[0];\n            dir->tail[1] = ((lfs_block_t*)attrs[i].buffer)[1];\n            dir->split = (lfs_tag_chunk(attrs[i].tag) & 1);\n            lfs_pair_fromle32(dir->tail);\n        }\n    }\n\n    // should we actually drop the directory block?\n    if (hasdelete && dir->count == 0) {\n        LFS_ASSERT(pdir);\n        int err = lfs_fs_pred(lfs, dir->pair, pdir);\n        if (err && err != LFS_ERR_NOENT) {\n            return err;\n        }\n\n        if (err != LFS_ERR_NOENT && pdir->split) {\n            state = LFS_OK_DROPPED;\n            goto fixmlist;\n        }\n    }\n\n    if (dir->erased) {\n        // try to commit\n        struct lfs_commit commit = {\n            .block = dir->pair[0],\n            .off = dir->off,\n            .ptag = dir->etag,\n            .crc = 0xffffffff,\n\n            .begin = dir->off,\n            .end = (lfs->cfg->metadata_max ?\n                lfs->cfg->metadata_max : lfs->cfg->block_size) - 8,\n        };\n\n        // traverse attrs that need to be written out\n        lfs_pair_tole32(dir->tail);\n        int err = lfs_dir_traverse(lfs,\n                dir, dir->off, dir->etag, attrs, attrcount,\n                0, 0, 0, 0, 0,\n                lfs_dir_commit_commit, &(struct lfs_dir_commit_commit){\n                    lfs, &commit});\n        lfs_pair_fromle32(dir->tail);\n        if (err) {\n            if (err == LFS_ERR_NOSPC || err == LFS_ERR_CORRUPT) {\n                goto compact;\n            }\n            return err;\n        }\n\n        // commit any global diffs if we have any\n        lfs_gstate_t delta = {0};\n        lfs_gstate_xor(&delta, &lfs->gstate);\n        lfs_gstate_xor(&delta, &lfs->gdisk);\n        lfs_gstate_xor(&delta, &lfs->gdelta);\n        delta.tag &= ~LFS_MKTAG(0, 0, 0x3ff);\n        if (!lfs_gstate_iszero(&delta)) {\n            err = lfs_dir_getgstate(lfs, dir, &delta);\n            if (err) {\n                return err;\n            }\n\n            lfs_gstate_tole32(&delta);\n            err = lfs_dir_commitattr(lfs, &commit,\n                    LFS_MKTAG(LFS_TYPE_MOVESTATE, 0x3ff,\n                        sizeof(delta)), &delta);\n            if (err) {\n                if (err == LFS_ERR_NOSPC || err == LFS_ERR_CORRUPT) {\n                    goto compact;\n                }\n                return err;\n            }\n        }\n\n        // finalize commit with the crc\n        err = lfs_dir_commitcrc(lfs, &commit);\n        if (err) {\n            if (err == LFS_ERR_NOSPC || err == LFS_ERR_CORRUPT) {\n                goto compact;\n            }\n            return err;\n        }\n\n        // successful commit, update dir\n        LFS_ASSERT(commit.off % lfs->cfg->prog_size == 0);\n        dir->off = commit.off;\n        dir->etag = commit.ptag;\n        // and update gstate\n        lfs->gdisk = lfs->gstate;\n        lfs->gdelta = (lfs_gstate_t){0};\n\n        goto fixmlist;\n    }\n\ncompact:\n    // fall back to compaction\n    lfs_cache_drop(lfs, &lfs->pcache);\n\n    state = lfs_dir_splittingcompact(lfs, dir, attrs, attrcount,\n            dir, 0, dir->count);\n    if (state < 0) {\n        return state;\n    }\n\n    goto fixmlist;\n\nfixmlist:;\n    // this complicated bit of logic is for fixing up any active\n    // metadata-pairs that we may have affected\n    //\n    // note we have to make two passes since the mdir passed to\n    // lfs_dir_commit could also be in this list, and even then\n    // we need to copy the pair so they don't get clobbered if we refetch\n    // our mdir.\n    lfs_block_t oldpair[2] = {pair[0], pair[1]};\n    for (struct lfs_mlist *d = lfs->mlist; d; d = d->next) {\n        if (lfs_pair_cmp(d->m.pair, oldpair) == 0) {\n            d->m = *dir;\n            if (d->m.pair != pair) {\n                for (int i = 0; i < attrcount; i++) {\n                    if (lfs_tag_type3(attrs[i].tag) == LFS_TYPE_DELETE &&\n                            d->id == lfs_tag_id(attrs[i].tag) &&\n                            d->type != LFS_TYPE_DIR) {\n                        d->m.pair[0] = LFS_BLOCK_NULL;\n                        d->m.pair[1] = LFS_BLOCK_NULL;\n                    } else if (lfs_tag_type3(attrs[i].tag) == LFS_TYPE_DELETE &&\n                            d->id > lfs_tag_id(attrs[i].tag)) {\n                        d->id -= 1;\n                        if (d->type == LFS_TYPE_DIR) {\n                            ((lfs_dir_t*)d)->pos -= 1;\n                        }\n                    } else if (lfs_tag_type3(attrs[i].tag) == LFS_TYPE_CREATE &&\n                            d->id >= lfs_tag_id(attrs[i].tag)) {\n                        d->id += 1;\n                        if (d->type == LFS_TYPE_DIR) {\n                            ((lfs_dir_t*)d)->pos += 1;\n                        }\n                    }\n                }\n            }\n\n            while (d->id >= d->m.count && d->m.split) {\n                // we split and id is on tail now\n                if (lfs_pair_cmp(d->m.tail, lfs->root) != 0) {\n                    d->id -= d->m.count;\n                }\n                int err = lfs_dir_fetch(lfs, &d->m, d->m.tail);\n                if (err) {\n                    return err;\n                }\n            }\n        }\n    }\n\n    return state;\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are close to the target function lfs_dir_relocatingcommit:\nstruct lfs_mattr {\n    lfs_tag_t tag;\n    const void *buffer;\n}\n...\ntypedef struct lfs_mdir {\n    lfs_block_t pair[2];\n    uint32_t rev;\n    lfs_off_t off;\n    uint32_t etag;\n    uint16_t count;\n    bool erased;\n    bool split;\n    lfs_block_t tail[2];\n} lfs_mdir_t\n...\ntypedef uint32_t lfs_block_t\n...\ntypedef struct lfs {\n    lfs_cache_t rcache;\n    lfs_cache_t pcache;\n\n    lfs_block_t root[2];\n    struct lfs_mlist {\n        struct lfs_mlist *next;\n        uint16_t id;\n        uint8_t type;\n        lfs_mdir_t m;\n    } *mlist;\n    uint32_t seed;\n\n    lfs_gstate_t gstate;\n    lfs_gstate_t gdisk;\n    lfs_gstate_t gdelta;\n\n    struct lfs_lookahead {\n        lfs_block_t start;\n        lfs_block_t size;\n        lfs_block_t next;\n        lfs_block_t ckpoint;\n        uint8_t *buffer;\n    } lookahead;\n\n    const struct lfs_config *cfg;\n    lfs_size_t block_count;\n    lfs_size_t name_max;\n    lfs_size_t file_max;\n    lfs_size_t attr_max;\n    lfs_size_t inline_max;\n\n#ifdef LFS_MIGRATE\n    struct lfs1 *lfs1;\n#endif\n} lfs_t\n...\nstruct lfs_config {\n    // Opaque user provided context that can be used to pass\n    // information to the block device operations\n    void *context;\n\n    // Read a region in a block. Negative error codes are propagated\n    // to the user.\n    int (*read)(const struct lfs_config *c, lfs_block_t block,\n            lfs_off_t off, void *buffer, lfs_size_t size);\n\n    // Program a region in a block. The block must have previously\n    // been erased. Negative error codes are propagated to the user.\n    // May return LFS_ERR_CORRUPT if the block should be considered bad.\n    int (*prog)(const struct lfs_config *c, lfs_block_t block,\n            lfs_off_t off, const void *buffer, lfs_size_t size);\n\n    // Erase a block. A block must be erased before being programmed.\n    // The state of an erased block is undefined. Negative error codes\n    // are propagated to the user.\n    // May return LFS_ERR_CORRUPT if the block should be considered bad.\n    int (*erase)(const struct lfs_config *c, lfs_block_t block);\n\n    // Sync the state of the underlying block device. Negative error codes\n    // are propagated to the user.\n    int (*sync)(const struct lfs_config *c);\n\n#ifdef LFS_THREADSAFE\n    // Lock the underlying block device. Negative error codes\n    // are propagated to the user.\n    int (*lock)(const struct lfs_config *c);\n\n    // Unlock the underlying block device. Negative error codes\n    // are propagated to the user.\n    int (*unlock)(const struct lfs_config *c);\n#endif\n\n    // Minimum size of a block read in bytes. All read operations will be a\n    // multiple of this value.\n    lfs_size_t read_size;\n\n    // Minimum size of a block program in bytes. All program operations will be\n    // a multiple of this value.\n    lfs_size_t prog_size;\n\n    // Size of an erasable block in bytes. This does not impact ram consumption\n    // and may be larger than the physical erase size. However, non-inlined\n    // files take up at minimum one block. Must be a multiple of the read and\n    // program sizes.\n    lfs_size_t block_size;\n\n    // Number of erasable blocks on the device. Defaults to block_count stored\n    // on disk when zero.\n    lfs_size_t block_count;\n\n    // Number of erase cycles before littlefs evicts metadata logs and moves\n    // the metadata to another block. Suggested values are in the\n    // range 100-1000, with large values having better performance at the cost\n    // of less consistent wear distribution.\n    //\n    // Set to -1 to disable block-level wear-leveling.\n    int32_t block_cycles;\n\n    // Size of block caches in bytes. Each cache buffers a portion of a block in\n    // RAM. The littlefs needs a read cache, a program cache, and one additional\n    // cache per file. Larger caches can improve performance by storing more\n    // data and reducing the number of disk accesses. Must be a multiple of the\n    // read and program sizes, and a factor of the block size.\n    lfs_size_t cache_size;\n\n    // Size of the lookahead buffer in bytes. A larger lookahead buffer\n    // increases the number of blocks found during an allocation pass. The\n    // lookahead buffer is stored as a compact bitmap, so each byte of RAM\n    // can track 8 blocks.\n    lfs_size_t lookahead_size;\n\n    // Threshold for metadata compaction during lfs_fs_gc in bytes. Metadata\n    // pairs that exceed this threshold will be compacted during lfs_fs_gc.\n    // Defaults to ~88% block_size when zero, though the default may change\n    // in the future.\n    //\n    // Note this only affects lfs_fs_gc. Normal compactions still only occur\n    // when full.\n    //\n    // Set to -1 to disable metadata compaction during lfs_fs_gc.\n    lfs_size_t compact_thresh;\n\n    // Optional statically allocated read buffer. Must be cache_size.\n    // By default lfs_malloc is used to allocate this buffer.\n    void *read_buffer;\n\n    // Optional statically allocated program buffer. Must be cache_size.\n    // By default lfs_malloc is used to allocate this buffer.\n    void *prog_buffer;\n\n    // Optional statically allocated lookahead buffer. Must be lookahead_size.\n    // By default lfs_malloc is used to allocate this buffer.\n    void *lookahead_buffer;\n\n    // Optional upper limit on length of file names in bytes. No downside for\n    // larger names except the size of the info struct which is controlled by\n    // the LFS_NAME_MAX define. Defaults to LFS_NAME_MAX or name_max stored on\n    // disk when zero.\n    lfs_size_t name_max;\n\n    // Optional upper limit on files in bytes. No downside for larger files\n    // but must be <= LFS_FILE_MAX. Defaults to LFS_FILE_MAX or file_max stored\n    // on disk when zero.\n    lfs_size_t file_max;\n\n    // Optional upper limit on custom attributes in bytes. No downside for\n    // larger attributes size but must be <= LFS_ATTR_MAX. Defaults to\n    // LFS_ATTR_MAX or attr_max stored on disk when zero.\n    lfs_size_t attr_max;\n\n    // Optional upper limit on total space given to metadata pairs in bytes. On\n    // devices with large blocks (e.g. 128kB) setting this to a low size (2-8kB)\n    // can help bound the metadata compaction time. Must be <= block_size.\n    // Defaults to block_size when zero.\n    lfs_size_t metadata_max;\n\n    // Optional upper limit on inlined files in bytes. Inlined files live in\n    // metadata and decrease storage requirements, but may be limited to\n    // improve metadata-related performance. Must be <= cache_size, <=\n    // attr_max, and <= block_size/8. Defaults to the largest possible\n    // inline_max when zero.\n    //\n    // Set to -1 to disable inlined files.\n    lfs_size_t inline_max;\n\n#ifdef LFS_MULTIVERSION\n    // On-disk version to use when writing in the form of 16-bit major version\n    // + 16-bit minor version. This limiting metadata to what is supported by\n    // older minor versions. Note that some features will be lost. Defaults to \n    // to the most recent minor version when zero.\n    uint32_t disk_version;\n#endif\n}\n...\nstruct lfs_mlist {\n        struct lfs_mlist *next;\n        uint16_t id;\n        uint8_t type;\n        lfs_mdir_t m;\n    }\n...\nstatic inline uint16_t lfs_tag_type3(lfs_tag_t tag) {\n    return (tag & 0x7ff00000) >> 20;\n}\n...\nstatic inline uint16_t lfs_tag_type1(lfs_tag_t tag) {\n    return (tag & 0x70000000) >> 20;\n}\n...\nstatic inline uint8_t lfs_tag_chunk(lfs_tag_t tag) {\n    return (tag & 0x0ff00000) >> 20;\n}\n...\nstatic inline uint16_t lfs_tag_id(lfs_tag_t tag) {\n    return (tag & 0x000ffc00) >> 10;\n}\n...\nstatic inline void lfs_pair_fromle32(lfs_block_t pair[2]) {\n    pair[0] = lfs_fromle32(pair[0]);\n    pair[1] = lfs_fromle32(pair[1]);\n}\n...\nstatic inline void lfs_pair_tole32(lfs_block_t pair[2]) {\n    pair[0] = lfs_tole32(pair[0]);\n    pair[1] = lfs_tole32(pair[1]);\n}\n...\nstatic inline int lfs_pair_cmp(\n        const lfs_block_t paira[2],\n        const lfs_block_t pairb[2]) {\n    return !(paira[0] == pairb[0] || paira[1] == pairb[1] ||\n             paira[0] == pairb[1] || paira[1] == pairb[0]);\n}\n...\nstatic inline bool lfs_pair_isnull(const lfs_block_t pair[2]) {\n    return pair[0] == LFS_BLOCK_NULL || pair[1] == LFS_BLOCK_NULL;\n}\n...\nstatic int lfs_fs_pred(lfs_t *lfs,\n        const lfs_block_t pair[2], lfs_mdir_t *pdir) {\n    // iterate over all directory directory entries\n    pdir->tail[0] = 0;\n    pdir->tail[1] = 1;\n    struct lfs_tortoise_t tortoise = {\n        .pair = {LFS_BLOCK_NULL, LFS_BLOCK_NULL},\n        .i = 1,\n        .period = 1,\n    };\n    int err = LFS_ERR_OK;\n    while (!lfs_pair_isnull(pdir->tail)) {\n        err = lfs_tortoise_detectcycles(pdir, &tortoise);\n        if (err < 0) {\n            return LFS_ERR_CORRUPT;\n        }\n\n        if (lfs_pair_cmp(pdir->tail, pair) == 0) {\n            return 0;\n        }\n\n        int err = lfs_dir_fetch(lfs, pdir, pdir->tail);\n        if (err) {\n            return err;\n        }\n    }\n\n    return LFS_ERR_NOENT;\n}\n...\nstatic int lfs_dir_traverse(lfs_t *lfs,\n        const lfs_mdir_t *dir, lfs_off_t off, lfs_tag_t ptag,\n        const struct lfs_mattr *attrs, int attrcount,\n        lfs_tag_t tmask, lfs_tag_t ttag,\n        uint16_t begin, uint16_t end, int16_t diff,\n        int (*cb)(void *data, lfs_tag_t tag, const void *buffer), void *data) {\n    // This function in inherently recursive, but bounded. To allow tool-based\n    // analysis without unnecessary code-cost we use an explicit stack\n    struct lfs_dir_traverse stack[LFS_DIR_TRAVERSE_DEPTH-1];\n    unsigned sp = 0;\n    int res;\n\n    // iterate over directory and attrs\n    lfs_tag_t tag;\n    const void *buffer;\n    struct lfs_diskoff disk = {0};\n    while (true) {\n        {\n            if (off+lfs_tag_dsize(ptag) < dir->off) {\n                off += lfs_tag_dsize(ptag);\n                int err = lfs_bd_read(lfs,\n                        NULL, &lfs->rcache, sizeof(tag),\n                        dir->pair[0], off, &tag, sizeof(tag));\n                if (err) {\n                    return err;\n                }\n\n                tag = (lfs_frombe32(tag) ^ ptag) | 0x80000000;\n                disk.block = dir->pair[0];\n                disk.off = off+sizeof(lfs_tag_t);\n                buffer = &disk;\n                ptag = tag;\n            } else if (attrcount > 0) {\n                tag = attrs[0].tag;\n                buffer = attrs[0].buffer;\n                attrs += 1;\n                attrcount -= 1;\n            } else {\n                // finished traversal, pop from stack?\n                res = 0;\n                break;\n            }\n\n            // do we need to filter?\n            lfs_tag_t mask = LFS_MKTAG(0x7ff, 0, 0);\n            if ((mask & tmask & tag) != (mask & tmask & ttag)) {\n                continue;\n            }\n\n            if (lfs_tag_id(tmask) != 0) {\n                LFS_ASSERT(sp < LFS_DIR_TRAVERSE_DEPTH);\n                // recurse, scan for duplicates, and update tag based on\n                // creates/deletes\n                stack[sp] = (struct lfs_dir_traverse){\n                    .dir        = dir,\n                    .off        = off,\n                    .ptag       = ptag,\n                    .attrs      = attrs,\n                    .attrcount  = attrcount,\n                    .tmask      = tmask,\n                    .ttag       = ttag,\n                    .begin      = begin,\n                    .end        = end,\n                    .diff       = diff,\n                    .cb         = cb,\n                    .data       = data,\n                    .tag        = tag,\n                    .buffer     = buffer,\n                    .disk       = disk,\n                };\n                sp += 1;\n\n                tmask = 0;\n                ttag = 0;\n                begin = 0;\n                end = 0;\n                diff = 0;\n                cb = lfs_dir_traverse_filter;\n                data = &stack[sp-1].tag;\n                continue;\n            }\n        }\n\npopped:\n        // in filter range?\n        if (lfs_tag_id(tmask) != 0 &&\n                !(lfs_tag_id(tag) >= begin && lfs_tag_id(tag) < end)) {\n            continue;\n        }\n\n        // handle special cases for mcu-side operations\n        if (lfs_tag_type3(tag) == LFS_FROM_NOOP) {\n            // do nothing\n        } else if (lfs_tag_type3(tag) == LFS_FROM_MOVE) {\n            // Without this condition, lfs_dir_traverse can exhibit an\n            // extremely expensive O(n^3) of nested loops when renaming.\n            // This happens because lfs_dir_traverse tries to filter tags by\n            // the tags in the source directory, triggering a second\n            // lfs_dir_traverse with its own filter operation.\n            //\n            // traverse with commit\n            // '-> traverse with filter\n            //     '-> traverse with move\n            //         '-> traverse with filter\n            //\n            // However we don't actually care about filtering the second set of\n            // tags, since duplicate tags have no effect when filtering.\n            //\n            // This check skips this unnecessary recursive filtering explicitly,\n            // reducing this runtime from O(n^3) to O(n^2).\n            if (cb == lfs_dir_traverse_filter) {\n                continue;\n            }\n\n            // recurse into move\n            stack[sp] = (struct lfs_dir_traverse){\n                .dir        = dir,\n                .off        = off,\n                .ptag       = ptag,\n                .attrs      = attrs,\n                .attrcount  = attrcount,\n                .tmask      = tmask,\n                .ttag       = ttag,\n                .begin      = begin,\n                .end        = end,\n                .diff       = diff,\n                .cb         = cb,\n                .data       = data,\n                .tag        = LFS_MKTAG(LFS_FROM_NOOP, 0, 0),\n            };\n            sp += 1;\n\n            uint16_t fromid = lfs_tag_size(tag);\n            uint16_t toid = lfs_tag_id(tag);\n            dir = buffer;\n            off = 0;\n            ptag = 0xffffffff;\n            attrs = NULL;\n            attrcount = 0;\n            tmask = LFS_MKTAG(0x600, 0x3ff, 0);\n            ttag = LFS_MKTAG(LFS_TYPE_STRUCT, 0, 0);\n            begin = fromid;\n            end = fromid+1;\n            diff = toid-fromid+diff;\n        } else if (lfs_tag_type3(tag) == LFS_FROM_USERATTRS) {\n            for (unsigned i = 0; i < lfs_tag_size(tag); i++) {\n                const struct lfs_attr *a = buffer;\n                res = cb(data, LFS_MKTAG(LFS_TYPE_USERATTR + a[i].type,\n                        lfs_tag_id(tag) + diff, a[i].size), a[i].buffer);\n                if (res < 0) {\n                    return res;\n                }\n\n                if (res) {\n                    break;\n                }\n            }\n        } else {\n            res = cb(data, tag + LFS_MKTAG(0, diff, 0), buffer);\n            if (res < 0) {\n                return res;\n            }\n\n            if (res) {\n                break;\n            }\n        }\n    }\n\n    if (sp > 0) {\n        // pop from the stack and return, fortunately all pops share\n        // a destination\n        dir         = stack[sp-1].dir;\n        off         = stack[sp-1].off;\n        ptag        = stack[sp-1].ptag;\n        attrs       = stack[sp-1].attrs;\n        attrcount   = stack[sp-1].attrcount;\n        tmask       = stack[sp-1].tmask;\n        ttag        = stack[sp-1].ttag;\n        begin       = stack[sp-1].begin;\n        end         = stack[sp-1].end;\n        diff        = stack[sp-1].diff;\n        cb          = stack[sp-1].cb;\n        data        = stack[sp-1].data;\n        tag         = stack[sp-1].tag;\n        buffer      = stack[sp-1].buffer;\n        disk        = stack[sp-1].disk;\n        sp -= 1;\n        goto popped;\n    } else {\n        return res;\n    }\n}\n...\nstatic int lfs_dir_commitattr(lfs_t *lfs, struct lfs_commit *commit,\n        lfs_tag_t tag, const void *buffer) {\n    // check if we fit\n    lfs_size_t dsize = lfs_tag_dsize(tag);\n    if (commit->off + dsize > commit->end) {\n        return LFS_ERR_NOSPC;\n    }\n\n    // write out tag\n    lfs_tag_t ntag = lfs_tobe32((tag & 0x7fffffff) ^ commit->ptag);\n    int err = lfs_dir_commitprog(lfs, commit, &ntag, sizeof(ntag));\n    if (err) {\n        return err;\n    }\n\n    if (!(tag & 0x80000000)) {\n        // from memory\n        err = lfs_dir_commitprog(lfs, commit, buffer, dsize-sizeof(tag));\n        if (err) {\n            return err;\n        }\n    } else {\n        // from disk\n        const struct lfs_diskoff *disk = buffer;\n        for (lfs_off_t i = 0; i < dsize-sizeof(tag); i++) {\n            // rely on caching to make this efficient\n            uint8_t dat;\n            err = lfs_bd_read(lfs,\n                    NULL, &lfs->rcache, dsize-sizeof(tag)-i,\n                    disk->block, disk->off+i, &dat, 1);\n            if (err) {\n                return err;\n            }\n\n            err = lfs_dir_commitprog(lfs, commit, &dat, 1);\n            if (err) {\n                return err;\n            }\n        }\n    }\n\n    commit->ptag = tag & 0x7fffffff;\n    return 0;\n}\n...\nstatic int lfs_dir_commitcrc(lfs_t *lfs, struct lfs_commit *commit) {\n    // align to program units\n    //\n    // this gets a bit complex as we have two types of crcs:\n    // - 5-word crc with fcrc to check following prog (middle of block)\n    // - 2-word crc with no following prog (end of block)\n    const lfs_off_t end = lfs_alignup(\n            lfs_min(commit->off + 5*sizeof(uint32_t), lfs->cfg->block_size),\n            lfs->cfg->prog_size);\n\n    lfs_off_t off1 = 0;\n    uint32_t crc1 = 0;\n\n    // create crc tags to fill up remainder of commit, note that\n    // padding is not crced, which lets fetches skip padding but\n    // makes committing a bit more complicated\n    while (commit->off < end) {\n        lfs_off_t noff = (\n                lfs_min(end - (commit->off+sizeof(lfs_tag_t)), 0x3fe)\n                + (commit->off+sizeof(lfs_tag_t)));\n        // too large for crc tag? need padding commits\n        if (noff < end) {\n            noff = lfs_min(noff, end - 5*sizeof(uint32_t));\n        }\n\n        // space for fcrc?\n        uint8_t eperturb = (uint8_t)-1;\n        if (noff >= end && noff <= lfs->cfg->block_size - lfs->cfg->prog_size) {\n            // first read the leading byte, this always contains a bit\n            // we can perturb to avoid writes that don't change the fcrc\n            int err = lfs_bd_read(lfs,\n                    NULL, &lfs->rcache, lfs->cfg->prog_size,\n                    commit->block, noff, &eperturb, 1);\n            if (err && err != LFS_ERR_CORRUPT) {\n                return err;\n            }\n\n        #ifdef LFS_MULTIVERSION\n            // unfortunately fcrcs break mdir fetching < lfs2.1, so only write\n            // these if we're a >= lfs2.1 filesystem\n            if (lfs_fs_disk_version(lfs) <= 0x00020000) {\n                // don't write fcrc\n            } else\n        #endif\n            {\n                // find the expected fcrc, don't bother avoiding a reread\n                // of the eperturb, it should still be in our cache\n                struct lfs_fcrc fcrc = {\n                    .size = lfs->cfg->prog_size,\n                    .crc = 0xffffffff\n                };\n                err = lfs_bd_crc(lfs,\n                        NULL, &lfs->rcache, lfs->cfg->prog_size,\n                        commit->block, noff, fcrc.size, &fcrc.crc);\n                if (err && err != LFS_ERR_CORRUPT) {\n                    return err;\n                }\n\n                lfs_fcrc_tole32(&fcrc);\n                err = lfs_dir_commitattr(lfs, commit,\n                        LFS_MKTAG(LFS_TYPE_FCRC, 0x3ff, sizeof(struct lfs_fcrc)),\n                        &fcrc);\n                if (err) {\n                    return err;\n                }\n            }\n        }\n\n        // build commit crc\n        struct {\n            lfs_tag_t tag;\n            uint32_t crc;\n        } ccrc;\n        lfs_tag_t ntag = LFS_MKTAG(\n                LFS_TYPE_CCRC + (((uint8_t)~eperturb) >> 7), 0x3ff,\n                noff - (commit->off+sizeof(lfs_tag_t)));\n        ccrc.tag = lfs_tobe32(ntag ^ commit->ptag);\n        commit->crc = lfs_crc(commit->crc, &ccrc.tag, sizeof(lfs_tag_t));\n        ccrc.crc = lfs_tole32(commit->crc);\n\n        int err = lfs_bd_prog(lfs,\n                &lfs->pcache, &lfs->rcache, false,\n                commit->block, commit->off, &ccrc, sizeof(ccrc));\n        if (err) {\n            return err;\n        }\n\n        // keep track of non-padding checksum to verify\n        if (off1 == 0) {\n            off1 = commit->off + sizeof(lfs_tag_t);\n            crc1 = commit->crc;\n        }\n\n        commit->off = noff;\n        // perturb valid bit?\n        commit->ptag = ntag ^ ((0x80UL & ~eperturb) << 24);\n        // reset crc for next commit\n        commit->crc = 0xffffffff;\n\n        // manually flush here since we don't prog the padding, this confuses\n        // the caching layer\n        if (noff >= end || noff >= lfs->pcache.off + lfs->cfg->cache_size) {\n            // flush buffers\n            int err = lfs_bd_sync(lfs, &lfs->pcache, &lfs->rcache, false);\n            if (err) {\n                return err;\n            }\n        }\n    }\n\n    // successful commit, check checksums to make sure\n    //\n    // note that we don't need to check padding commits, worst\n    // case if they are corrupted we would have had to compact anyways\n    lfs_off_t off = commit->begin;\n    uint32_t crc = 0xffffffff;\n    int err = lfs_bd_crc(lfs,\n            NULL, &lfs->rcache, off1+sizeof(uint32_t),\n            commit->block, off, off1-off, &crc);\n    if (err) {\n        return err;\n    }\n\n    // check non-padding commits against known crc\n    if (crc != crc1) {\n        return LFS_ERR_CORRUPT;\n    }\n\n    // make sure to check crc in case we happen to pick\n    // up an unrelated crc (frozen block?)\n    err = lfs_bd_crc(lfs,\n            NULL, &lfs->rcache, sizeof(uint32_t),\n            commit->block, off1, sizeof(uint32_t), &crc);\n    if (err) {\n        return err;\n    }\n\n    if (crc != 0) {\n        return LFS_ERR_CORRUPT;\n    }\n\n    return 0;\n}\n...\nstatic int lfs_dir_getgstate(lfs_t *lfs, const lfs_mdir_t *dir,\n        lfs_gstate_t *gstate) {\n    lfs_gstate_t temp;\n    lfs_stag_t res = lfs_dir_get(lfs, dir, LFS_MKTAG(0x7ff, 0, 0),\n            LFS_MKTAG(LFS_TYPE_MOVESTATE, 0, sizeof(temp)), &temp);\n    if (res < 0 && res != LFS_ERR_NOENT) {\n        return res;\n    }\n\n    if (res != LFS_ERR_NOENT) {\n        // xor together to find resulting gstate\n        lfs_gstate_fromle32(&temp);\n        lfs_gstate_xor(gstate, &temp);\n    }\n\n    return 0;\n}\n...\nstatic int lfs_dir_splittingcompact(lfs_t *lfs, lfs_mdir_t *dir,\n        const struct lfs_mattr *attrs, int attrcount,\n        lfs_mdir_t *source, uint16_t begin, uint16_t end) {\n    while (true) {\n        // find size of first split, we do this by halving the split until\n        // the metadata is guaranteed to fit\n        //\n        // Note that this isn't a true binary search, we never increase the\n        // split size. This may result in poorly distributed metadata but isn't\n        // worth the extra code size or performance hit to fix.\n        lfs_size_t split = begin;\n        while (end - split > 1) {\n            lfs_size_t size = 0;\n            int err = lfs_dir_traverse(lfs,\n                    source, 0, 0xffffffff, attrs, attrcount,\n                    LFS_MKTAG(0x400, 0x3ff, 0),\n                    LFS_MKTAG(LFS_TYPE_NAME, 0, 0),\n                    split, end, -split,\n                    lfs_dir_commit_size, &size);\n            if (err) {\n                return err;\n            }\n\n            // space is complicated, we need room for:\n            //\n            // - tail:         4+2*4 = 12 bytes\n            // - gstate:       4+3*4 = 16 bytes\n            // - move delete:  4     = 4 bytes\n            // - crc:          4+4   = 8 bytes\n            //                 total = 40 bytes\n            //\n            // And we cap at half a block to avoid degenerate cases with\n            // nearly-full metadata blocks.\n            //\n            lfs_size_t metadata_max = (lfs->cfg->metadata_max)\n                    ? lfs->cfg->metadata_max\n                    : lfs->cfg->block_size;\n            if (end - split < 0xff\n                    && size <= lfs_min(\n                        metadata_max - 40,\n                        lfs_alignup(\n                            metadata_max/2,\n                            lfs->cfg->prog_size))) {\n                break;\n            }\n\n            split = split + ((end - split) / 2);\n        }\n\n        if (split == begin) {\n            // no split needed\n            break;\n        }\n\n        // split into two metadata pairs and continue\n        int err = lfs_dir_split(lfs, dir, attrs, attrcount,\n                source, split, end);\n        if (err && err != LFS_ERR_NOSPC) {\n            return err;\n        }\n\n        if (err) {\n            // we can't allocate a new block, try to compact with degraded\n            // performance\n            LFS_WARN(\"Unable to split {0x%\"PRIx32\", 0x%\"PRIx32\"}\",\n                    dir->pair[0], dir->pair[1]);\n            break;\n        } else {\n            end = split;\n        }\n    }\n\n    if (lfs_dir_needsrelocation(lfs, dir)\n            && lfs_pair_cmp(dir->pair, (const lfs_block_t[2]){0, 1}) == 0) {\n        // oh no! we're writing too much to the superblock,\n        // should we expand?\n        lfs_ssize_t size = lfs_fs_size_(lfs);\n        if (size < 0) {\n            return size;\n        }\n\n        // littlefs cannot reclaim expanded superblocks, so expand cautiously\n        //\n        // if our filesystem is more than ~88% full, don't expand, this is\n        // somewhat arbitrary\n        if (lfs->block_count - size > lfs->block_count/8) {\n            LFS_DEBUG(\"Expanding superblock at rev %\"PRIu32, dir->rev);\n            int err = lfs_dir_split(lfs, dir, attrs, attrcount,\n                    source, begin, end);\n            if (err && err != LFS_ERR_NOSPC) {\n                return err;\n            }\n\n            if (err) {\n                // welp, we tried, if we ran out of space there's not much\n                // we can do, we'll error later if we've become frozen\n                LFS_WARN(\"Unable to expand superblock\");\n            } else {\n                // duplicate the superblock entry into the new superblock\n                end = 1;\n            }\n        }\n    }\n\n    return lfs_dir_compact(lfs, dir, attrs, attrcount, source, begin, end);\n}\n...\nstatic int lfs_dir_fetch(lfs_t *lfs,\n        lfs_mdir_t *dir, const lfs_block_t pair[2]) {\n    // note, mask=-1, tag=-1 can never match a tag since this\n    // pattern has the invalid bit set\n    return (int)lfs_dir_fetchmatch(lfs, dir, pair,\n            (lfs_tag_t)-1, (lfs_tag_t)-1, NULL, NULL, NULL);\n}\n...\nstatic int lfs_bd_read(lfs_t *lfs,\n        const lfs_cache_t *pcache, lfs_cache_t *rcache, lfs_size_t hint,\n        lfs_block_t block, lfs_off_t off,\n        void *buffer, lfs_size_t size) {\n    uint8_t *data = buffer;\n    if (off+size > lfs->cfg->block_size\n            || (lfs->block_count && block >= lfs->block_count)) {\n        return LFS_ERR_CORRUPT;\n    }\n\n    while (size > 0) {\n        lfs_size_t diff = size;\n\n        if (pcache && block == pcache->block &&\n                off < pcache->off + pcache->size) {\n            if (off >= pcache->off) {\n                // is already in pcache?\n                diff = lfs_min(diff, pcache->size - (off-pcache->off));\n                memcpy(data, &pcache->buffer[off-pcache->off], diff);\n\n                data += diff;\n                off += diff;\n                size -= diff;\n                continue;\n            }\n\n            // pcache takes priority\n            diff = lfs_min(diff, pcache->off-off);\n        }\n\n        if (block == rcache->block &&\n                off < rcache->off + rcache->size) {\n            if (off >= rcache->off) {\n                // is already in rcache?\n                diff = lfs_min(diff, rcache->size - (off-rcache->off));\n                memcpy(data, &rcache->buffer[off-rcache->off], diff);\n\n                data += diff;\n                off += diff;\n                size -= diff;\n                continue;\n            }\n\n            // rcache takes priority\n            diff = lfs_min(diff, rcache->off-off);\n        }\n\n        if (size >= hint && off % lfs->cfg->read_size == 0 &&\n                size >= lfs->cfg->read_size) {\n            // bypass cache?\n            diff = lfs_aligndown(diff, lfs->cfg->read_size);\n            int err = lfs->cfg->read(lfs->cfg, block, off, data, diff);\n            if (err) {\n                return err;\n            }\n\n            data += diff;\n            off += diff;\n            size -= diff;\n            continue;\n        }\n\n        // load to cache, first condition can no longer fail\n        LFS_ASSERT(!lfs->block_count || block < lfs->block_count);\n        rcache->block = block;\n        rcache->off = lfs_aligndown(off, lfs->cfg->read_size);\n        rcache->size = lfs_min(\n                lfs_min(\n                    lfs_alignup(off+hint, lfs->cfg->read_size),\n                    lfs->cfg->block_size)\n                - rcache->off,\n                lfs->cfg->cache_size);\n        int err = lfs->cfg->read(lfs->cfg, rcache->block,\n                rcache->off, rcache->buffer, rcache->size);\n        LFS_ASSERT(err <= 0);\n        if (err) {\n            return err;\n        }\n    }\n\n    return 0;\n}\n...\nstatic int lfs_bd_prog(lfs_t *lfs,\n        lfs_cache_t *pcache, lfs_cache_t *rcache, bool validate,\n        lfs_block_t block, lfs_off_t off,\n        const void *buffer, lfs_size_t size) {\n    const uint8_t *data = buffer;\n    LFS_ASSERT(block == LFS_BLOCK_INLINE || block < lfs->block_count);\n    LFS_ASSERT(off + size <= lfs->cfg->block_size);\n\n    while (size > 0) {\n        if (block == pcache->block &&\n                off >= pcache->off &&\n                off < pcache->off + lfs->cfg->cache_size) {\n            // already fits in pcache?\n            lfs_size_t diff = lfs_min(size,\n                    lfs->cfg->cache_size - (off-pcache->off));\n            memcpy(&pcache->buffer[off-pcache->off], data, diff);\n\n            data += diff;\n            off += diff;\n            size -= diff;\n\n            pcache->size = lfs_max(pcache->size, off - pcache->off);\n            if (pcache->size == lfs->cfg->cache_size) {\n                // eagerly flush out pcache if we fill up\n                int err = lfs_bd_flush(lfs, pcache, rcache, validate);\n                if (err) {\n                    return err;\n                }\n            }\n\n            continue;\n        }\n\n        // pcache must have been flushed, either by programming and\n        // entire block or manually flushing the pcache\n        LFS_ASSERT(pcache->block == LFS_BLOCK_NULL);\n\n        // prepare pcache, first condition can no longer fail\n        pcache->block = block;\n        pcache->off = lfs_aligndown(off, lfs->cfg->prog_size);\n        pcache->size = 0;\n    }\n\n    return 0;\n}\n...\nstatic int lfs_bd_sync(lfs_t *lfs,\n        lfs_cache_t *pcache, lfs_cache_t *rcache, bool validate) {\n    lfs_cache_drop(lfs, rcache);\n\n    int err = lfs_bd_flush(lfs, pcache, rcache, validate);\n    if (err) {\n        return err;\n    }\n\n    err = lfs->cfg->sync(lfs->cfg);\n    LFS_ASSERT(err <= 0);\n    return err;\n}\n...\nstatic int lfs_bd_crc(lfs_t *lfs,\n        const lfs_cache_t *pcache, lfs_cache_t *rcache, lfs_size_t hint,\n        lfs_block_t block, lfs_off_t off, lfs_size_t size, uint32_t *crc) {\n    lfs_size_t diff = 0;\n\n    for (lfs_off_t i = 0; i < size; i += diff) {\n        uint8_t dat[8];\n        diff = lfs_min(size-i, sizeof(dat));\n        int err = lfs_bd_read(lfs,\n                pcache, rcache, hint-i,\n                block, off+i, &dat, diff);\n        if (err) {\n            return err;\n        }\n\n        *crc = lfs_crc(*crc, &dat, diff);\n    }\n\n    return 0;\n}\n...\nstatic inline void lfs_cache_drop(lfs_t *lfs, lfs_cache_t *rcache) {\n    // do not zero, cheaper if cache is readonly or only going to be\n    // written with identical data (during relocates)\n    (void)lfs;\n    rcache->block = LFS_BLOCK_NULL;\n}\n...\ntypedef struct lfs_gstate {\n    uint32_t tag;\n    lfs_block_t pair[2];\n} lfs_gstate_t\n...\nstatic inline void lfs_gstate_xor(lfs_gstate_t *a, const lfs_gstate_t *b) {\n    a->tag ^= b->tag;\n    a->pair[0] ^= b->pair[0];\n    a->pair[1] ^= b->pair[1];\n}\n...\nstatic inline bool lfs_gstate_iszero(const lfs_gstate_t *a) {\n    return a->tag == 0\n            && a->pair[0] == 0\n            && a->pair[1] == 0;\n}\n...\nstatic inline void lfs_gstate_tole32(lfs_gstate_t *a) {\n    a->tag     = lfs_tole32(a->tag);\n    a->pair[0] = lfs_tole32(a->pair[0]);\n    a->pair[1] = lfs_tole32(a->pair[1]);\n}\n...\n#define LFS_MKTAG(type, id, size) (((lfs_tag_t)(type)<<20)|((lfs_tag_t)(id)<<10)|(lfs_tag_t)(size))\n...\n#define LFS_ASSERT(test) assert(test)\n...\n#define LFS_BLOCK_NULL ((lfs_block_t)-1)\n...\nstatic int lfs_dir_commit_commit(void *p, lfs_tag_t tag, const void *buffer) {\n    struct lfs_dir_commit_commit *commit = p;\n    return lfs_dir_commitattr(commit->lfs, commit->commit, tag, buffer);\n}\n</additional code>\n<target_code>\nHere is the code fragment that you need to analyze:\n```c\nmemcpy(data, &pcache->buffer[off-pcache->off], diff);\n```\nFocus on the above code fragment and judge whether it violates the rule 21.17 or not.\n</target_code>\n",
  "is_rule_violated": false,
  "scenario": "No violation: prior checks constrain the copy to within pcache->buffer. The code is inside \"if (off >= pcache->off)\" and diff was computed as diff = lfs_min(diff, pcache->size - (off - pcache->off)); so (off - pcache->off) < pcache->size and diff <= pcache->size - (off - pcache->off). Therefore memcpy(data, &pcache->buffer[off-pcache->off], diff) cannot read beyond the bounds of pcache->buffer (assuming pcache->buffer has capacity pcache->size and pcache is valid)."
}