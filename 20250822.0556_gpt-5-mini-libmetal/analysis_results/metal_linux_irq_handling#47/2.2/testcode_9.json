{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 2.2 is: A project shall not contain dead code.\nAny operation that is executed but whose removal would not affect program behaviour constitutes dead code. Notes:\n1. The behaviour of an embedded sytem is often determined not just by the nature of its actions, but also by the time at which they occur.\n2. unreachable code is not dead code, as it cannot be executed.\n3.Initialization is not the same as an assignment operation and is therfore not a candidate for dead code.\n</misra_rule>\n\nAnother agent reported that the target code violated the Rule 2.2.\nHere is the comment from the agent.\n<violation scenario>\nThe memset(pfds, 0, MAX_IRQS * sizeof(struct pollfd)); is redundant (dead). Before poll() is called, the code explicitly initializes every pfds entry that will be used: pfds[0] and the subsequent pfds[j] entries filled inside the bitmap loop. Removing the memset would not change runtime behaviour, so it constitutes executed but unnecessary (dead) code. Note also a related bug: the allocation is for FD_SETSIZE elements but the memset uses MAX_IRQS (FD_SETSIZE-1), so the last element can remain uninitialized when all IRQs are enabled \u2014 this underscores that the memset is both unnecessary and incorrectly sized.\n</violation scenario>\n\nYour job is to insert logging code to capture the control flow and state information of the program.\nInsert logging code so that other agents can reproduce and get evidence for the violation scenario.\n<response format>\n- test_code: a modified metal_linux_irq_handling code that contains useful probe functions that print useful logs to a file \"/tmp/logs/472.txt\". The logs should contain the control path and state information of the violation scenario. If test code is not required to validate the scenario, you can leave this blank.\nHere are the constraints for the test code:\n- The test code MUST preserve the original logic of metal_linux_irq_handling as much as possible.\nYou CAN just insert probe functions to the original code.\n- DO NOT introduce new violations to demonstrate your scenario.\n- The test code MUST be compilable and executable.\n- The test code MUST include line breaks.\n- You MUST NOT define the main function.\n- You MUST NOT define macros.\n</response format>\n<target_code>\nHere is the code of metal_linux_irq_handling:\n```c\nstatic void *metal_linux_irq_handling(void *args)\n{\n\tstruct sched_param param;\n\tuint64_t val;\n\tint ret;\n\tint i, j, pfds_total;\n\tstruct pollfd *pfds;\n\n\t(void)args;\n\n\tpfds = (struct pollfd *)malloc(FD_SETSIZE * sizeof(struct pollfd));\n\tif (!pfds) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to allocate irq fds mem.\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tparam.sched_priority = sched_get_priority_max(SCHED_FIFO);\n\t/* Ignore the set scheduler error */\n\tret = sched_setscheduler(0, SCHED_FIFO, &param);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_WARNING,\n\t\t\t  \"%s: Failed to set scheduler: %s.\\n\", __func__,\n\t\t\t  strerror(ret));\n\t}\n\n\twhile (1) {\n\t\tmetal_mutex_acquire(&irq_lock);\n\t\tif (irq_handling_stop) {\n\t\t\t/* Killing this IRQ handling thread */\n\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Get the fdset */\n\t\tmemset(pfds, 0, MAX_IRQS * sizeof(struct pollfd));\n\t\tpfds[0].fd = irq_notify_fd;\n\t\tpfds[0].events = POLLIN;\n\t\tj = 1;\n\t\tmetal_bitmap_for_each_set_bit(irqs_enabled, i,\n\t\t\t\t\t      linux_irq_cntr.irq_num) {\n\t\t\tpfds[j].fd = i;\n\t\t\tpfds[j].events = POLLIN;\n\t\t\tj++;\n\t\t}\n\t\tmetal_mutex_release(&irq_lock);\n\t\t/* Wait for interrupt */\n\t\tret = poll(pfds, j, -1);\n\t\tif (ret < 0) {\n\t\t\tmetal_log(METAL_LOG_ERROR, \"%s: poll() failed: %s.\\n\",\n\t\t\t\t  __func__, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\t/* Waken up from interrupt */\n\t\tpfds_total = j;\n\t\tfor (i = 0; i < pfds_total; i++) {\n\t\t\tif ((pfds[i].fd == irq_notify_fd) &&\n\t\t\t    (pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\t/* IRQ registration change notification */\n\t\t\t\tif (read(pfds[i].fd,\n\t\t\t\t\t (void *)&val, sizeof(uint64_t)) < 0)\n\t\t\t\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t\t\t\t  \"%s, read irq fd %d failed\\n\",\n\t\t\t\t\t\t  __func__, pfds[i].fd);\n\t\t\t} else if ((pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\tstruct metal_device *dev = NULL;\n\t\t\t\tint irq_handled = 0;\n\t\t\t\tint fd;\n\n\t\t\t\tfd = pfds[i].fd;\n\t\t\t\tdev = irqs_devs[fd];\n\t\t\t\tmetal_mutex_acquire(&irq_lock);\n\t\t\t\tif (metal_irq_handle(&irqs[fd], fd)\n\t\t\t\t    == METAL_IRQ_HANDLED)\n\t\t\t\t\tirq_handled = 1;\n\t\t\t\tif (irq_handled) {\n\t\t\t\t\tif (dev && dev->bus->ops.dev_irq_ack)\n\t\t\t\t\t\tdev->bus->ops.dev_irq_ack(\n\t\t\t\t\t\t\tdev->bus, dev, fd);\n\t\t\t\t}\n\t\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\t} else if (pfds[i].revents) {\n\t\t\t\tmetal_log(METAL_LOG_DEBUG,\n\t\t\t\t\t  \"%s: poll unexpected. fd %d: %d\\n\",\n\t\t\t\t\t  __func__,\n\t\t\t\t\t  pfds[i].fd, pfds[i].revents);\n\t\t\t}\n\t\t}\n\t}\n\tfree(pfds);\n\treturn NULL;\n}\n```\n</target_code>\nPlease provide the modified metal_linux_irq_handling code.\n",
  "test_code": "static void *metal_linux_irq_handling(void *args)\n{\n\tstruct sched_param param;\n\tuint64_t val;\n\tint ret;\n\tint i, j, pfds_total;\n\tstruct pollfd *pfds;\n\n\t(void)args;\n\n\t/* Probe logging helper: append diagnostic lines to /tmp/logs/472.txt */\n\tstatic void log_probe(const char *fmt, ...)\n\t{\n\t\tFILE *f = fopen(\"/tmp/logs/472.txt\", \"a\");\n\t\tif (!f)\n\t\t\treturn;\n\t\tva_list ap;\n\t\tva_start(ap, fmt);\n\t\tvfprintf(f, fmt, ap);\n\t\tva_end(ap);\n\t\tfprintf(f, \"\\n\");\n\t\tfflush(f);\n\t\tfclose(f);\n\t}\n\n\tpfds = (struct pollfd *)malloc(FD_SETSIZE * sizeof(struct pollfd));\n\tif (!pfds) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to allocate irq fds mem.\\n\", __func__);\n\t\tlog_probe(\"[probe] %s: malloc failed for %d entries (size %zu)\", __func__, FD_SETSIZE, (size_t)(FD_SETSIZE * sizeof(struct pollfd)));\n\t\treturn NULL;\n\t}\n\n\t/* Log allocation and configuration facts to help reproduce the reported issue */\n\tlog_probe(\"[probe] %s: allocated pfds=%p entries=%d entry_size=%zu total_bytes=%zu\", __func__, (void *)pfds, FD_SETSIZE, sizeof(struct pollfd), (size_t)(FD_SETSIZE * sizeof(struct pollfd)));\n\tlog_probe(\"[probe] %s: MAX_IRQS=%d, memset_bytes=%zu\", __func__, MAX_IRQS, (size_t)(MAX_IRQS * sizeof(struct pollfd)));\n\n\tparam.sched_priority = sched_get_priority_max(SCHED_FIFO);\n\t/* Ignore the set scheduler error */\n\tret = sched_setscheduler(0, SCHED_FIFO, &param);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_WARNING,\n\t\t\t  \"%s: Failed to set scheduler: %s.\\n\", __func__,\n\t\t\t  strerror(ret));\n\t\tlog_probe(\"[probe] %s: sched_setscheduler returned %d\", __func__, ret);\n\t}\n\n\twhile (1) {\n\t\tmetal_mutex_acquire(&irq_lock);\n\t\tif (irq_handling_stop) {\n\t\t\t/* Killing this IRQ handling thread */\n\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Get the fdset */\n\t\t/* Log state immediately before the memset call */\n\t\tlog_probe(\"[probe] %s: about to memset pfds=%p with bytes=%zu (MAX_IRQS=%d) alloc_entries=%d\", __func__, (void *)pfds, (size_t)(MAX_IRQS * sizeof(struct pollfd)), MAX_IRQS, FD_SETSIZE);\n\t\tmemset(pfds, 0, MAX_IRQS * sizeof(struct pollfd));\n\t\tlog_probe(\"[probe] %s: memset done. note: memset_bytes=%zu alloc_bytes=%zu\", __func__, (size_t)(MAX_IRQS * sizeof(struct pollfd)), (size_t)(FD_SETSIZE * sizeof(struct pollfd)));\n\n\t\tpfds[0].fd = irq_notify_fd;\n\t\tpfds[0].events = POLLIN;\n\t\tj = 1;\n\t\tmetal_bitmap_for_each_set_bit(irqs_enabled, i,\n\t\t\t\t\t  linux_irq_cntr.irq_num) {\n\t\t\tpfds[j].fd = i;\n\t\t\tpfds[j].events = POLLIN;\n\t\t\tj++;\n\t\t}\n\n\t\t/* Log which pfds entries were explicitly initialized */\n\t\tlog_probe(\"[probe] %s: after init loop: j=%d (number of irq fds = %d), linux_irq_cntr.irq_num=%d\", __func__, j, j - 1, linux_irq_cntr.irq_num);\n\t\tfor (i = 0; i < j; i++) {\n\t\t\t/* These entries have been explicitly initialized; safe to read */\n\t\t\tlog_probe(\"[probe] %s: pfds[%d] = { fd=%d, events=0x%x }\", __func__, i, pfds[i].fd, pfds[i].events);\n\t\t}\n\n\t\t/* Warn if memset region was smaller than the allocation: evidence of incorrect sizing */\n\t\tif ((size_t)(MAX_IRQS * sizeof(struct pollfd)) < (size_t)(FD_SETSIZE * sizeof(struct pollfd))) {\n\t\t\tlog_probe(\"[probe] %s: WARNING: memset covered fewer bytes (%zu) than allocation (%zu). Uninitialized pfds indices from %d..%d may exist.\",\n\t\t\t\t__func__, (size_t)(MAX_IRQS * sizeof(struct pollfd)), (size_t)(FD_SETSIZE * sizeof(struct pollfd)), MAX_IRQS, FD_SETSIZE - 1);\n\t\t}\n\n\t\t/* Additional check: if the number of pfds passed to poll() exceeds the memseted count, this may exercise uninitialized memory if those entries were not explicitly set */\n\t\tif (j > MAX_IRQS) {\n\t\t\tlog_probe(\"[probe] %s: NOTICE: pfds count j=%d is greater than MAX_IRQS=%d (memset count). poll() will be called with j=%d.\", __func__, j, MAX_IRQS, j);\n\t\t}\n\n\t\tmetal_mutex_release(&irq_lock);\n\t\t/* Wait for interrupt */\n\t\tret = poll(pfds, j, -1);\n\t\tif (ret < 0) {\n\t\t\tmetal_log(METAL_LOG_ERROR, \"%s: poll() failed: %s.\\n\",\n\t\t\t\t  __func__, strerror(errno));\n\t\t\tlog_probe(\"[probe] %s: poll() failed with errno=%d (%s)\", __func__, errno, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\t/* Waken up from interrupt */\n\t\tpfds_total = j;\n\t\tfor (i = 0; i < pfds_total; i++) {\n\t\t\tif ((pfds[i].fd == irq_notify_fd) &&\n\t\t\t    (pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\t/* IRQ registration change notification */\n\t\t\t\tif (read(pfds[i].fd,\n\t\t\t\t\t (void *)&val, sizeof(uint64_t)) < 0)\n\t\t\t\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t\t\t\t  \"%s, read irq fd %d failed\\n\",\n\t\t\t\t\t\t  __func__, pfds[i].fd);\n\t\t\t\tlog_probe(\"[probe] %s: irq_notify_fd event on fd=%d revents=0x%x\", __func__, pfds[i].fd, pfds[i].revents);\n\t\t\t} else if ((pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\tstruct metal_device *dev = NULL;\n\t\t\t\tint irq_handled = 0;\n\t\t\t\tint fd;\n\n\t\t\t\tfd = pfds[i].fd;\n\t\t\t\tdev = irqs_devs[fd];\n\t\t\t\tlog_probe(\"[probe] %s: handling fd=%d revents=0x%x\", __func__, fd, pfds[i].revents);\n\t\t\t\tmetal_mutex_acquire(&irq_lock);\n\t\t\t\tif (metal_irq_handle(&irqs[fd], fd)\n\t\t\t\t    == METAL_IRQ_HANDLED)\n\t\t\t\t\tirq_handled = 1;\n\t\t\t\tif (irq_handled) {\n\t\t\t\t\tif (dev && dev->bus->ops.dev_irq_ack)\n\t\t\t\t\t\tdev->bus->ops.dev_irq_ack(\n\t\t\t\t\t\t\tdev->bus, dev, fd);\n\t\t\t\t}\n\t\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\t} else if (pfds[i].revents) {\n\t\t\t\tmetal_log(METAL_LOG_DEBUG,\n\t\t\t\t\t  \"%s: poll unexpected. fd %d: %d\\n\",\n\t\t\t\t\t  __func__,\n\t\t\t\t\t  pfds[i].fd, pfds[i].revents);\n\t\t\t\tlog_probe(\"[probe] %s: poll unexpected for fd=%d revents=0x%x\", __func__, pfds[i].fd, pfds[i].revents);\n\t\t\t}\n\t\t}\n\t}\n\tfree(pfds);\n\tlog_probe(\"[probe] %s: exiting and freed pfds=%p\", __func__, (void *)pfds);\n\treturn NULL;\n}\n"
}