{
  "user_message": "An agent A claimed that the following mhd_tool_get_proc_cpu_count code violates the MISRA C 2023 rule 2.1.\n<misra_rule>\nMISRA C 2023 Rule 2.1 is A project shall not contain unreachable code.\n\n</misra_rule>\n<target_code>\nHere is the original code of mhd_tool_get_proc_cpu_count.\n```c\nint\nmhd_tool_get_proc_cpu_count (void)\n{\n  int res;\n\n#if defined(__linux__) || defined(__GLIBC__)\n  /* On Linux kernel try first 'sched_getaffinity()' as it should be\n     the native API.\n     Also try it first on other kernels if Glibc is used. */\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_cpuset_getaffinity_ ();\n  if (0 < res)\n    return res;\n#else  /* ! __linux__ && ! __GLIBC__ */\n  /* On non-Linux kernels 'cpuset_getaffinity()' could be the native API,\n     while 'sched_getaffinity()' could be implemented in compatibility layer. */\n  res = mhd_tool_get_proc_cpu_count_cpuset_getaffinity_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_ ();\n  if (0 < res)\n    return res;\n#endif /* ! __linux__ && ! __GLIBC__ */\n\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_np_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_w32_ ();\n  if (0 < res)\n    return res;\n\n  return -1;\n}\n```\n</target_code>\n<response format>\nProvide your answer with these fields.\n- explanation: your evaluation of the scenario provided by the agent A.\n- is_report_correct: whether the report is correct or not (true/false). True means there is a violation.\n</response format>\n<scenario>\nThe agent A provided the following violation scenario:\nThe code fragment is enclosed in a preprocessor conditional '#if 0' which is a constant false condition, so the entire block is never part of any translation unit and cannot be executed. This is effectively unreachable (disabled) code and therefore violates MISRA C 2023 Rule 2.1.\n</scenario>\n<additional code>Here are the definitions of symbols that are close to the target function mhd_tool_get_proc_cpu_count:\nstatic int\nmhd_tool_get_proc_cpu_count_sched_getaffinity_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_SCHED_GETAFFINITY) && defined(HAVE_GETPID)\n  /* Glibc style */\n  if (0 >= ret)\n  {\n    cpu_set_t cur_set;\n    if (0 == sched_getaffinity (getpid (), sizeof (cur_set), &cur_set))\n    {\n#ifdef HAVE_CPU_COUNT\n      ret = CPU_COUNT (&cur_set);\n#else  /* ! HAVE_CPU_COUNT */\n      unsigned int i;\n      ret = 0;\n      for (i = 0; i < CPU_SETSIZE_SAFE; ++i)\n      {\n        if (CPU_ISSET (i, &cur_set))\n          ++ret;\n      }\n      if (0 == ret)\n        ret = -1;\n#endif /* ! HAVE_CPU_COUNT */\n    }\n  }\n#ifdef HAVE_CPU_COUNT_S\n  if (0 >= ret)\n  {\n    /* Use 256 times larger size than size for default maximum CPU number.\n       Hopefully it would be enough even for exotic situations. */\n    static const unsigned int set_size_cpus = 256 * CPU_SETSIZE;\n    const size_t set_size_bytes = CPU_ALLOC_SIZE (set_size_cpus);\n    cpu_set_t *p_set;\n\n    p_set = CPU_ALLOC (set_size_cpus);\n    if (NULL != p_set)\n    {\n      if (0 == sched_getaffinity (getpid (), set_size_bytes, p_set))\n      {\n#ifndef MHD_FUNC_CPU_COUNT_S_GETS_CPUS\n        ret = CPU_COUNT_S (set_size_bytes, p_set);\n#else  /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n        ret = CPU_COUNT_S (set_size_cpus, p_set);\n#endif /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n      }\n      CPU_FREE (p_set);\n    }\n  }\n#endif /* HAVE_CPU_COUNT_S */\n#endif /* HAVE_SCHED_GETAFFINITY && HAVE_GETPID */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_cpuset_getaffinity_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_CPUSET_GETAFFINITY)\n  /* FreeBSD style */\n  if (0 >= ret)\n  {\n    cpuset_t cur_mask;\n    /* The should get \"anonymous\" mask/set. The anonymous mask is always\n       a subset of the assigned set (which is a subset of the root set). */\n    if (0 == cpuset_getaffinity (CPU_LEVEL_WHICH, CPU_WHICH_PID, (id_t) -1,\n                                 sizeof (cur_mask), &cur_mask))\n    {\n#ifdef HAVE_CPU_COUNT\n      ret = CPU_COUNT (&cur_mask);\n#else  /* ! HAVE_CPU_COUNT */\n      unsigned int i;\n      ret = 0;\n      for (i = 0; i < CPU_SETSIZE_SAFE; ++i)\n      {\n        if (CPU_ISSET (i, &cur_mask))\n          ++ret;\n      }\n      if (0 == ret)\n        ret = -1;\n#endif /* ! HAVE_CPU_COUNT */\n    }\n  }\n#ifdef HAVE_CPU_COUNT_S\n  if (0 >= ret)\n  {\n    /* Use 256 times larger size than size for default maximum CPU number.\n       Hopefully it would be enough even for exotic situations. */\n    static const unsigned int mask_size_cpus = 256 * CPU_SETSIZE;\n    const size_t mask_size_bytes = CPU_ALLOC_SIZE (mask_size_cpus);\n    cpuset_t *p_mask;\n\n    p_mask = CPU_ALLOC (mask_size_cpus);\n    if (NULL != p_mask)\n    {\n      if (0 == cpuset_getaffinity (CPU_LEVEL_WHICH, CPU_WHICH_PID, (id_t) -1,\n                                   mask_size_bytes, p_mask))\n      {\n#ifndef MHD_FUNC_CPU_COUNT_S_GETS_CPUS\n        ret = CPU_COUNT_S (mask_size_bytes, p_mask);\n#else  /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n        ret = CPU_COUNT_S (mask_size_cpus, p_mask);\n#endif /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n      }\n      CPU_FREE (p_mask);\n    }\n  }\n#endif /* HAVE_CPU_COUNT_S */\n#endif /* HAVE_CPUSET_GETAFFINITY */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_sched_getaffinity_np_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_SCHED_GETAFFINITY_NP) && defined(HAVE_GETPID)\n  /* NetBSD style */\n  cpuset_t *cpuset_ptr;\n  cpuset_ptr = cpuset_create ();\n  if (NULL != cpuset_ptr)\n  {\n    if (0 == sched_getaffinity_np (getpid (), cpuset_size (cpuset_ptr),\n                                   cpuset_ptr))\n    {\n      cpuid_t cpu_num;\n#if defined(HAVE_SYSCONF) && defined(HAVE_DECL__SC_NPROCESSORS_CONF)\n      unsigned int max_num = 0;\n      long sc_value;\n      sc_value = sysconf (_SC_NPROCESSORS_ONLN);\n      if (0 < sc_value)\n        max_num = (unsigned int) sc_value;\n      if (0 < max_num)\n      {\n        ret = 0;\n        for (cpu_num = 0; cpu_num < max_num; ++cpu_num)\n          if (0 < cpuset_isset (cpu_num, cpuset_ptr))\n            ++ret;\n      }\n      else /* Combined with the next 'if' */\n#endif /* HAVE_SYSCONF && HAVE_DECL__SC_NPROCESSORS_CONF */\n      if (1)\n      {\n        int res;\n        cpu_num = 0;\n        ret = 0;\n        do\n        {\n          res = cpuset_isset (cpu_num++, cpuset_ptr);\n          if (0 < res)\n            ++ret;\n        } while (0 <= res);\n      }\n#ifdef __NetBSD__\n      if (0 == ret)\n      {\n        /* On NetBSD \"unset\" affinity (exactly zero CPUs) means\n           \"all CPUs are available\". */\n        ret = mhd_tool_get_sys_cpu_count_sysctl_ ();\n      }\n#endif /* __NetBSD__ */\n    }\n    cpuset_destroy (cpuset_ptr);\n  }\n#endif /* HAVE_SCHED_GETAFFINITY_NP && HAVE_GETPID */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_w32_ (void)\n{\n  int ret = -1;\n#if defined(_WIN32) && ! defined(__CYGWIN__)\n  /* W32 Native */\n  /**\n   * Maximum used number of CPU groups.\n   * Improvement: Implement dynamic allocation when it would be reasonable\n   */\n#define MHDT_MAX_GROUP_COUNT 128\n  /**\n   * The count of logical CPUs as returned by GetProcessAffinityMask()\n   */\n  int count_by_proc_aff_mask;\n  count_by_proc_aff_mask = -1;\n  if (1)\n  {\n    DWORD_PTR proc_aff;\n    DWORD_PTR sys_aff;\n\n    if (GetProcessAffinityMask (GetCurrentProcess (), &proc_aff, &sys_aff))\n    {\n      /* Count all set bits */\n      for (count_by_proc_aff_mask = 0; 0 != proc_aff; proc_aff &= proc_aff - 1)\n        ++count_by_proc_aff_mask;\n    }\n  }\n  if (0 < count_by_proc_aff_mask)\n  {\n    HMODULE k32hndl;\n    k32hndl = LoadLibraryA (\"kernel32.dll\");\n    if (NULL != k32hndl)\n    {\n      typedef BOOL (WINAPI *GPGA_PTR)(HANDLE hProcess,\n                                      PUSHORT GroupCount,\n                                      PUSHORT GroupArray);\n      GPGA_PTR ptrGetProcessGroupAffinity;\n      ptrGetProcessGroupAffinity =\n        (GPGA_PTR) (void *) GetProcAddress (k32hndl,\n                                            \"GetProcessGroupAffinity\");\n      if (NULL == ptrGetProcessGroupAffinity)\n      {\n        /* Windows version before Win7 */\n        /* No processor groups supported, the process affinity mask gives full picture */\n        ret = count_by_proc_aff_mask;\n      }\n      else\n      {\n        /* Windows version Win7 or later */\n        /* Processor groups are supported */\n        USHORT arr_elements = MHDT_MAX_GROUP_COUNT;\n        USHORT groups_arr[MHDT_MAX_GROUP_COUNT]; /* Hopefully should be enough */\n        /* Improvement: Implement dynamic allocation when it would be reasonable */\n        /**\n         * Exactly one processor group is assigned to the process\n         */\n        bool single_cpu_group_assigned; /**< Exactly one processor group is assigned to the process */\n        struct mhdt_GR_AFFINITY\n        {\n          KAFFINITY Mask;\n          WORD Group;\n          WORD Reserved[3];\n        };\n        typedef BOOL (WINAPI *GPDCSM_PTR)(HANDLE Process,\n                                          struct mhdt_GR_AFFINITY *CpuSetMasks,\n                                          USHORT CpuSetMaskCount,\n                                          USHORT *RequiredMaskCount);\n        GPDCSM_PTR ptrGetProcessDefaultCpuSetMasks;\n        bool win_fe_or_later;\n        bool cpu_set_mask_assigned;\n\n        single_cpu_group_assigned = false;\n        if (ptrGetProcessGroupAffinity (GetCurrentProcess (), &arr_elements,\n                                        groups_arr))\n        {\n          if (1 == arr_elements)\n          {\n            /* Exactly one processor group assigned to the process */\n            single_cpu_group_assigned = true;\n#if 0 /* Disabled code */\n            /* The value returned by GetThreadGroupAffinity() is not relevant as\n               for the new threads the process affinity mask is used. */\n            ULONG_PTR proc_aff2;\n            typedef BOOL (WINAPI *GTGA_PTR)(HANDLE hThread,\n                                            struct mhdt_GR_AFFINITY *\n                                            GroupAffinity);\n            GTGA_PTR ptrGetThreadGroupAffinity;\n            ptrGetThreadGroupAffinity =\n              (GTGA_PTR) (void *) GetProcAddress (k32hndl,\n                                                  \"GetThreadGroupAffinity\");\n            if (NULL != ptrGetThreadGroupAffinity)\n            {\n              struct mhdt_GR_AFFINITY thr_gr_aff;\n              if (ptrGetThreadGroupAffinity (GetCurrentThread (), &thr_gr_aff))\n                proc_aff2 = (ULONG_PTR) thr_gr_aff.Mask;\n            }\n#endif /* Disabled code */\n          }\n        }\n        ptrGetProcessDefaultCpuSetMasks =\n          (GPDCSM_PTR) (void *) GetProcAddress (k32hndl,\n                                                \"GetProcessDefaultCpuSetMasks\");\n        if (NULL != ptrGetProcessDefaultCpuSetMasks)\n        {\n          /* This is Iron Release / Codename Fe\n             (also know as Windows 11 and Windows Server 2022)\n             or later version */\n          struct mhdt_GR_AFFINITY gr_affs[MHDT_MAX_GROUP_COUNT]; /* Hopefully should be enough */\n          /* Improvement: Implement dynamic allocation when it would be reasonable */\n          USHORT num_elm;\n\n          win_fe_or_later = true;\n\n          if (ptrGetProcessDefaultCpuSetMasks (GetCurrentProcess (), gr_affs,\n                                               sizeof (gr_affs)\n                                               / sizeof (gr_affs[0]), &num_elm))\n          {\n            if (0 == num_elm)\n            {\n              /* No group mask set */\n              cpu_set_mask_assigned = false;\n            }\n            else\n              cpu_set_mask_assigned = true;\n          }\n          else\n            cpu_set_mask_assigned = true; /* Assume the worst case */\n        }\n        else\n        {\n          win_fe_or_later = false;\n          cpu_set_mask_assigned = false;\n        }\n        if (! win_fe_or_later)\n        {\n          /* The OS is not capable of distributing threads across different\n             processor groups. Results reported by GetProcessAffinityMask()\n             are relevant for the main processor group for the process. */\n          ret = count_by_proc_aff_mask;\n        }\n        else\n        {\n          /* The of is capable of automatic threads distribution across\n             processor groups. */\n          if (cpu_set_mask_assigned)\n          {\n            /* Assigned Default CpuSet Masks combines with \"classic\"\n               affinity in the not fully clear way. The combination\n               is not documented and this functionality could be changed\n               any moment. */\n            ret = -1;\n          }\n          else\n          {\n            if (! single_cpu_group_assigned)\n            {\n              /* This is a multi processor group process on Win11 (or later).\n                 Each processor group may have different affinity and\n                 the OS has not API to get it.\n                 For example, affinity to the main processor group could be\n                 assigned by SetProcessAffinityMask() function, which converts\n                 the process to the single-processor-group type, but if\n                 SetThreadGroupAffinity() is called later and bind the thread\n                 to another processor group, the process becomes multi-processor-\n                 group again, however the initial affinity mask is still used\n                 for the initial (main) processor group. There is no API to read\n                 it.\n                 It is also possible that processor groups have different number\n                 of processors. */\n              ret = -1;\n            }\n            else\n            {\n              /* Single-processor-group process on Win11 (or later) without\n                 assigned Default CpuSet Masks. */\n              ret = count_by_proc_aff_mask;\n            }\n          }\n        }\n      }\n      FreeLibrary (k32hndl);\n    }\n  }\n#endif /* _WIN32 && ! __CYGWIN__ */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n</additional code>\n\n<instruction>\nJudge whether the report is correct or not.\n</instruction>\n",
  "is_report_correct": false,
  "explanation": "The report is not correct. The mhd_tool_get_proc_cpu_count function as shown has no enclosing '#if 0' that would remove the whole function from every translation unit; it uses normal conditional compilation (#if defined(__linux__) || defined(__GLIBC__)) to choose which API to try first. The only '#if 0' in the supplied additional code is a small disabled snippet inside the Windows-specific helper (mhd_tool_get_proc_cpu_count_w32_), which is a preprocessor-excluded region and therefore not part of the translation unit. MISRA Rule 2.1 forbids unreachable code in the program\u2019s control flow; code removed by the preprocessor is not part of the compiled program and hence is not an unreachable branch in the compiled control flow. In short, the agent\u2019s scenario (the function is enclosed in '#if 0' so it is unreachable) is factually incorrect for the provided target code, so the reported violation is not valid."
}