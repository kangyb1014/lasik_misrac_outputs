{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 2.2 is: A project shall not contain dead code.\nAny operation that is executed but whose removal would not affect program behaviour constitutes dead code. Notes:\n1. The behaviour of an embedded sytem is often determined not just by the nature of its actions, but also by the time at which they occur.\n2. unreachable code is not dead code, as it cannot be executed.\n3.Initialization is not the same as an assignment operation and is therfore not a candidate for dead code.\n</misra_rule>\n<response format>\nProvide your answer with these fields.\n- is_rule_violated: whether the rule is violated or not (true/false),\n- scenario: a specific violation scenario.\n</response format>\n<target_code>\nHere is the code of mhd_tool_get_proc_cpu_count:\n```c\nint\nmhd_tool_get_proc_cpu_count (void)\n{\n  int res;\n\n#if defined(__linux__) || defined(__GLIBC__)\n  /* On Linux kernel try first 'sched_getaffinity()' as it should be\n     the native API.\n     Also try it first on other kernels if Glibc is used. */\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_cpuset_getaffinity_ ();\n  if (0 < res)\n    return res;\n#else  /* ! __linux__ && ! __GLIBC__ */\n  /* On non-Linux kernels 'cpuset_getaffinity()' could be the native API,\n     while 'sched_getaffinity()' could be implemented in compatibility layer. */\n  res = mhd_tool_get_proc_cpu_count_cpuset_getaffinity_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_ ();\n  if (0 < res)\n    return res;\n#endif /* ! __linux__ && ! __GLIBC__ */\n\n  res = mhd_tool_get_proc_cpu_count_sched_getaffinity_np_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_proc_cpu_count_w32_ ();\n  if (0 < res)\n    return res;\n\n  return -1;\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are close to the target function mhd_tool_get_proc_cpu_count:\nstatic int\nmhd_tool_get_proc_cpu_count_sched_getaffinity_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_SCHED_GETAFFINITY) && defined(HAVE_GETPID)\n  /* Glibc style */\n  if (0 >= ret)\n  {\n    cpu_set_t cur_set;\n    if (0 == sched_getaffinity (getpid (), sizeof (cur_set), &cur_set))\n    {\n#ifdef HAVE_CPU_COUNT\n      ret = CPU_COUNT (&cur_set);\n#else  /* ! HAVE_CPU_COUNT */\n      unsigned int i;\n      ret = 0;\n      for (i = 0; i < CPU_SETSIZE_SAFE; ++i)\n      {\n        if (CPU_ISSET (i, &cur_set))\n          ++ret;\n      }\n      if (0 == ret)\n        ret = -1;\n#endif /* ! HAVE_CPU_COUNT */\n    }\n  }\n#ifdef HAVE_CPU_COUNT_S\n  if (0 >= ret)\n  {\n    /* Use 256 times larger size than size for default maximum CPU number.\n       Hopefully it would be enough even for exotic situations. */\n    static const unsigned int set_size_cpus = 256 * CPU_SETSIZE;\n    const size_t set_size_bytes = CPU_ALLOC_SIZE (set_size_cpus);\n    cpu_set_t *p_set;\n\n    p_set = CPU_ALLOC (set_size_cpus);\n    if (NULL != p_set)\n    {\n      if (0 == sched_getaffinity (getpid (), set_size_bytes, p_set))\n      {\n#ifndef MHD_FUNC_CPU_COUNT_S_GETS_CPUS\n        ret = CPU_COUNT_S (set_size_bytes, p_set);\n#else  /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n        ret = CPU_COUNT_S (set_size_cpus, p_set);\n#endif /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n      }\n      CPU_FREE (p_set);\n    }\n  }\n#endif /* HAVE_CPU_COUNT_S */\n#endif /* HAVE_SCHED_GETAFFINITY && HAVE_GETPID */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_cpuset_getaffinity_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_CPUSET_GETAFFINITY)\n  /* FreeBSD style */\n  if (0 >= ret)\n  {\n    cpuset_t cur_mask;\n    /* The should get \"anonymous\" mask/set. The anonymous mask is always\n       a subset of the assigned set (which is a subset of the root set). */\n    if (0 == cpuset_getaffinity (CPU_LEVEL_WHICH, CPU_WHICH_PID, (id_t) -1,\n                                 sizeof (cur_mask), &cur_mask))\n    {\n#ifdef HAVE_CPU_COUNT\n      ret = CPU_COUNT (&cur_mask);\n#else  /* ! HAVE_CPU_COUNT */\n      unsigned int i;\n      ret = 0;\n      for (i = 0; i < CPU_SETSIZE_SAFE; ++i)\n      {\n        if (CPU_ISSET (i, &cur_mask))\n          ++ret;\n      }\n      if (0 == ret)\n        ret = -1;\n#endif /* ! HAVE_CPU_COUNT */\n    }\n  }\n#ifdef HAVE_CPU_COUNT_S\n  if (0 >= ret)\n  {\n    /* Use 256 times larger size than size for default maximum CPU number.\n       Hopefully it would be enough even for exotic situations. */\n    static const unsigned int mask_size_cpus = 256 * CPU_SETSIZE;\n    const size_t mask_size_bytes = CPU_ALLOC_SIZE (mask_size_cpus);\n    cpuset_t *p_mask;\n\n    p_mask = CPU_ALLOC (mask_size_cpus);\n    if (NULL != p_mask)\n    {\n      if (0 == cpuset_getaffinity (CPU_LEVEL_WHICH, CPU_WHICH_PID, (id_t) -1,\n                                   mask_size_bytes, p_mask))\n      {\n#ifndef MHD_FUNC_CPU_COUNT_S_GETS_CPUS\n        ret = CPU_COUNT_S (mask_size_bytes, p_mask);\n#else  /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n        ret = CPU_COUNT_S (mask_size_cpus, p_mask);\n#endif /* MHD_FUNC_CPU_COUNT_S_GETS_CPUS */\n      }\n      CPU_FREE (p_mask);\n    }\n  }\n#endif /* HAVE_CPU_COUNT_S */\n#endif /* HAVE_CPUSET_GETAFFINITY */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_sched_getaffinity_np_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_SCHED_GETAFFINITY_NP) && defined(HAVE_GETPID)\n  /* NetBSD style */\n  cpuset_t *cpuset_ptr;\n  cpuset_ptr = cpuset_create ();\n  if (NULL != cpuset_ptr)\n  {\n    if (0 == sched_getaffinity_np (getpid (), cpuset_size (cpuset_ptr),\n                                   cpuset_ptr))\n    {\n      cpuid_t cpu_num;\n#if defined(HAVE_SYSCONF) && defined(HAVE_DECL__SC_NPROCESSORS_CONF)\n      unsigned int max_num = 0;\n      long sc_value;\n      sc_value = sysconf (_SC_NPROCESSORS_ONLN);\n      if (0 < sc_value)\n        max_num = (unsigned int) sc_value;\n      if (0 < max_num)\n      {\n        ret = 0;\n        for (cpu_num = 0; cpu_num < max_num; ++cpu_num)\n          if (0 < cpuset_isset (cpu_num, cpuset_ptr))\n            ++ret;\n      }\n      else /* Combined with the next 'if' */\n#endif /* HAVE_SYSCONF && HAVE_DECL__SC_NPROCESSORS_CONF */\n      if (1)\n      {\n        int res;\n        cpu_num = 0;\n        ret = 0;\n        do\n        {\n          res = cpuset_isset (cpu_num++, cpuset_ptr);\n          if (0 < res)\n            ++ret;\n        } while (0 <= res);\n      }\n#ifdef __NetBSD__\n      if (0 == ret)\n      {\n        /* On NetBSD \"unset\" affinity (exactly zero CPUs) means\n           \"all CPUs are available\". */\n        ret = mhd_tool_get_sys_cpu_count_sysctl_ ();\n      }\n#endif /* __NetBSD__ */\n    }\n    cpuset_destroy (cpuset_ptr);\n  }\n#endif /* HAVE_SCHED_GETAFFINITY_NP && HAVE_GETPID */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic int\nmhd_tool_get_proc_cpu_count_w32_ (void)\n{\n  int ret = -1;\n#if defined(_WIN32) && ! defined(__CYGWIN__)\n  /* W32 Native */\n  /**\n   * Maximum used number of CPU groups.\n   * Improvement: Implement dynamic allocation when it would be reasonable\n   */\n#define MHDT_MAX_GROUP_COUNT 128\n  /**\n   * The count of logical CPUs as returned by GetProcessAffinityMask()\n   */\n  int count_by_proc_aff_mask;\n  count_by_proc_aff_mask = -1;\n  if (1)\n  {\n    DWORD_PTR proc_aff;\n    DWORD_PTR sys_aff;\n\n    if (GetProcessAffinityMask (GetCurrentProcess (), &proc_aff, &sys_aff))\n    {\n      /* Count all set bits */\n      for (count_by_proc_aff_mask = 0; 0 != proc_aff; proc_aff &= proc_aff - 1)\n        ++count_by_proc_aff_mask;\n    }\n  }\n  if (0 < count_by_proc_aff_mask)\n  {\n    HMODULE k32hndl;\n    k32hndl = LoadLibraryA (\"kernel32.dll\");\n    if (NULL != k32hndl)\n    {\n      typedef BOOL (WINAPI *GPGA_PTR)(HANDLE hProcess,\n                                      PUSHORT GroupCount,\n                                      PUSHORT GroupArray);\n      GPGA_PTR ptrGetProcessGroupAffinity;\n      ptrGetProcessGroupAffinity =\n        (GPGA_PTR) (void *) GetProcAddress (k32hndl,\n                                            \"GetProcessGroupAffinity\");\n      if (NULL == ptrGetProcessGroupAffinity)\n      {\n        /* Windows version before Win7 */\n        /* No processor groups supported, the process affinity mask gives full picture */\n        ret = count_by_proc_aff_mask;\n      }\n      else\n      {\n        /* Windows version Win7 or later */\n        /* Processor groups are supported */\n        USHORT arr_elements = MHDT_MAX_GROUP_COUNT;\n        USHORT groups_arr[MHDT_MAX_GROUP_COUNT]; /* Hopefully should be enough */\n        /* Improvement: Implement dynamic allocation when it would be reasonable */\n        /**\n         * Exactly one processor group is assigned to the process\n         */\n        bool single_cpu_group_assigned; /**< Exactly one processor group is assigned to the process */\n        struct mhdt_GR_AFFINITY\n        {\n          KAFFINITY Mask;\n          WORD Group;\n          WORD Reserved[3];\n        };\n        typedef BOOL (WINAPI *GPDCSM_PTR)(HANDLE Process,\n                                          struct mhdt_GR_AFFINITY *CpuSetMasks,\n                                          USHORT CpuSetMaskCount,\n                                          USHORT *RequiredMaskCount);\n        GPDCSM_PTR ptrGetProcessDefaultCpuSetMasks;\n        bool win_fe_or_later;\n        bool cpu_set_mask_assigned;\n\n        single_cpu_group_assigned = false;\n        if (ptrGetProcessGroupAffinity (GetCurrentProcess (), &arr_elements,\n                                        groups_arr))\n        {\n          if (1 == arr_elements)\n          {\n            /* Exactly one processor group assigned to the process */\n            single_cpu_group_assigned = true;\n#if 0 /* Disabled code */\n            /* The value returned by GetThreadGroupAffinity() is not relevant as\n               for the new threads the process affinity mask is used. */\n            ULONG_PTR proc_aff2;\n            typedef BOOL (WINAPI *GTGA_PTR)(HANDLE hThread,\n                                            struct mhdt_GR_AFFINITY *\n                                            GroupAffinity);\n            GTGA_PTR ptrGetThreadGroupAffinity;\n            ptrGetThreadGroupAffinity =\n              (GTGA_PTR) (void *) GetProcAddress (k32hndl,\n                                                  \"GetThreadGroupAffinity\");\n            if (NULL != ptrGetThreadGroupAffinity)\n            {\n              struct mhdt_GR_AFFINITY thr_gr_aff;\n              if (ptrGetThreadGroupAffinity (GetCurrentThread (), &thr_gr_aff))\n                proc_aff2 = (ULONG_PTR) thr_gr_aff.Mask;\n            }\n#endif /* Disabled code */\n          }\n        }\n        ptrGetProcessDefaultCpuSetMasks =\n          (GPDCSM_PTR) (void *) GetProcAddress (k32hndl,\n                                                \"GetProcessDefaultCpuSetMasks\");\n        if (NULL != ptrGetProcessDefaultCpuSetMasks)\n        {\n          /* This is Iron Release / Codename Fe\n             (also know as Windows 11 and Windows Server 2022)\n             or later version */\n          struct mhdt_GR_AFFINITY gr_affs[MHDT_MAX_GROUP_COUNT]; /* Hopefully should be enough */\n          /* Improvement: Implement dynamic allocation when it would be reasonable */\n          USHORT num_elm;\n\n          win_fe_or_later = true;\n\n          if (ptrGetProcessDefaultCpuSetMasks (GetCurrentProcess (), gr_affs,\n                                               sizeof (gr_affs)\n                                               / sizeof (gr_affs[0]), &num_elm))\n          {\n            if (0 == num_elm)\n            {\n              /* No group mask set */\n              cpu_set_mask_assigned = false;\n            }\n            else\n              cpu_set_mask_assigned = true;\n          }\n          else\n            cpu_set_mask_assigned = true; /* Assume the worst case */\n        }\n        else\n        {\n          win_fe_or_later = false;\n          cpu_set_mask_assigned = false;\n        }\n        if (! win_fe_or_later)\n        {\n          /* The OS is not capable of distributing threads across different\n             processor groups. Results reported by GetProcessAffinityMask()\n             are relevant for the main processor group for the process. */\n          ret = count_by_proc_aff_mask;\n        }\n        else\n        {\n          /* The of is capable of automatic threads distribution across\n             processor groups. */\n          if (cpu_set_mask_assigned)\n          {\n            /* Assigned Default CpuSet Masks combines with \"classic\"\n               affinity in the not fully clear way. The combination\n               is not documented and this functionality could be changed\n               any moment. */\n            ret = -1;\n          }\n          else\n          {\n            if (! single_cpu_group_assigned)\n            {\n              /* This is a multi processor group process on Win11 (or later).\n                 Each processor group may have different affinity and\n                 the OS has not API to get it.\n                 For example, affinity to the main processor group could be\n                 assigned by SetProcessAffinityMask() function, which converts\n                 the process to the single-processor-group type, but if\n                 SetThreadGroupAffinity() is called later and bind the thread\n                 to another processor group, the process becomes multi-processor-\n                 group again, however the initial affinity mask is still used\n                 for the initial (main) processor group. There is no API to read\n                 it.\n                 It is also possible that processor groups have different number\n                 of processors. */\n              ret = -1;\n            }\n            else\n            {\n              /* Single-processor-group process on Win11 (or later) without\n                 assigned Default CpuSet Masks. */\n              ret = count_by_proc_aff_mask;\n            }\n          }\n        }\n      }\n      FreeLibrary (k32hndl);\n    }\n  }\n#endif /* _WIN32 && ! __CYGWIN__ */\n  if (0 >= ret)\n    return -1;\n  return ret;\n}\n...\nstatic unsigned int\ndetect_cpu_core_count (void)\n{\n  int sys_cpu_count;\n  sys_cpu_count = mhd_tool_get_system_cpu_count ();\n  if (0 >= sys_cpu_count)\n  {\n    int proc_cpu_count;\n    fprintf (stderr, \"Failed to detect the number of logical CPU cores \"\n             \"available on the system.\\n\");\n    proc_cpu_count = mhd_tool_get_proc_cpu_count ();\n    if (0 < proc_cpu_count)\n    {\n      fprintf (stderr, \"The number of CPU cores available for this process \"\n               \"is used as a fallback.\\n\");\n      sys_cpu_count = proc_cpu_count;\n    }\n#ifdef MHD_REAL_CPU_COUNT\n    if (0 >= sys_cpu_count)\n    {\n      fprintf (stderr, \"configure-detected hardcoded number is used \"\n               \"as a fallback.\\n\");\n      sys_cpu_count = MHD_REAL_CPU_COUNT;\n    }\n#endif\n    if (0 >= sys_cpu_count)\n      sys_cpu_count = 1;\n    printf (\"Assuming %d logical CPU core%s on this system.\\n\", sys_cpu_count,\n            (1 == sys_cpu_count) ? \"\" : \"s\");\n  }\n  else\n  {\n    printf (\"Detected %d logical CPU core%s on this system.\\n\", sys_cpu_count,\n            (1 == sys_cpu_count) ? \"\" : \"s\");\n  }\n  return (unsigned int) sys_cpu_count;\n}\n...\nstatic unsigned int\ndetect_process_cpu_core_count (void)\n{\n  unsigned int num_proc_cpu_cores;\n  unsigned int sys_cpu_cores;\n  int res;\n\n  sys_cpu_cores = get_cpu_core_count ();\n  res = mhd_tool_get_proc_cpu_count ();\n  if (0 > res)\n  {\n    fprintf (stderr, \"Cannot detect the number of logical CPU cores available \"\n             \"for this process.\\n\");\n    if (1 != sys_cpu_cores)\n      printf (\"Assuming all %u system logical CPU cores are available to run \"\n              \"threads of this process.\\n\", sys_cpu_cores);\n    else\n      printf (\"Assuming single logical CPU core available for this process.\\n\");\n    num_proc_cpu_cores = sys_cpu_cores;\n  }\n  else\n  {\n    printf (\"Detected %d logical CPU core%s available to run threads \"\n            \"of this process.\\n\", res, (1 == res) ? \"\" : \"s\");\n    num_proc_cpu_cores = (unsigned int) res;\n  }\n  if (num_proc_cpu_cores > sys_cpu_cores)\n  {\n    fprintf (stderr, \"WARNING: Detected number of CPU cores available \"\n             \"for this process (%u) is larger than detected number \"\n             \"of CPU cores on the system (%u).\\n\",\n             num_proc_cpu_cores, sys_cpu_cores);\n    num_proc_cpu_cores = sys_cpu_cores;\n    fprintf (stderr, \"Using %u as the number of logical CPU cores available \"\n             \"for this process.\\n\", num_proc_cpu_cores);\n  }\n  return num_proc_cpu_cores;\n}\n...\nint\nmhd_tool_get_system_cpu_count (void)\n{\n  int res;\n\n  /* Try specialised APIs first */\n  res = mhd_tool_get_sys_cpu_count_special_api_ ();\n  if (0 < res)\n    return res;\n\n  /* Try sysctl*(). This is typically a direct interface to\n     kernel values. */\n  res = mhd_tool_get_sys_cpu_count_sysctl_ ();\n  if (0 < res)\n    return res;\n\n  /* Try sysconf() as the last resort as this is a generic interface\n     which can be implemented by parsing system files. */\n  res = mhd_tool_get_sys_cpu_count_sysconf_ ();\n#if ! defined(__linux__) && ! defined(__GLIBC__)\n  if (0 < res)\n    return res;\n#else  /* __linux__ || __GLIBC__ */\n  if (2 < res)\n    return res;\n  if (0 < res)\n  {\n    /* '1' or '2' could a be fallback number.\n     * See get_nprocs_fallback() in glibc\n       sysdeps/unix/sysv/linux/getsysstats.c */\n\n    int proc_cpu_count;\n\n    proc_cpu_count = mhd_tool_get_proc_cpu_count ();\n    if (proc_cpu_count == res)\n    {\n      /* The detected number of CPUs available for the process\n         is equal to the detected number of system CPUs.\n         Assume detected number is correct. */\n      return res;\n    }\n  }\n#endif /* __linux__ || __GLIBC__  */\n\n  /* Try available fallbacks */\n\n  res = mhd_tool_get_sys_cpu_count_sysctl_fallback_ ();\n  if (0 < res)\n    return res;\n\n  res = mhd_tool_get_sys_cpu_count_sysconf_fallback_ ();\n#if ! defined(__linux__) && ! defined(__GLIBC__)\n  if (0 < res)\n    return res;\n#else  /* __linux__ || __GLIBC__ */\n  if (2 < res)\n    return res;\n#endif /* __linux__ || __GLIBC__  */\n\n  return -1; /* Cannot detect */\n}\n...\nstatic unsigned int\nget_cpu_core_count (void)\n{\n  static unsigned int num_cpu_cores = 0;\n  if (0 == num_cpu_cores)\n    num_cpu_cores = detect_cpu_core_count ();\n  return num_cpu_cores;\n}\n...\nstatic unsigned int\nget_process_cpu_core_count (void)\n{\n  static unsigned int proc_num_cpu_cores = 0;\n  if (0 == proc_num_cpu_cores)\n    proc_num_cpu_cores = detect_process_cpu_core_count ();\n  return proc_num_cpu_cores;\n}\n</additional code>\n<target_code>\nHere is the code fragment that you need to analyze:\n```c\nstatic int\nmhd_tool_get_proc_cpu_count_sched_getaffinity_np_ (void)\n{\n  int ret = -1;\n#if defined(HAVE_SCHED_GETAFFINITY_NP) && defined(HAVE_GETPID)\n  /* NetBSD style */\n  cpuset_t *cpuset_ptr;\n  cpuset_ptr = cpuset_create ();\n  if (NULL != cpuset_ptr)\n  {\n    if (0 == sched_getaffinity_np (getpid (), cpuset_size (cpuset_ptr),\n                                   cpuset_ptr))\n    {\n      cpuid_t cpu_num;\n#if defined(HAVE_SYSCONF) && defined(HAVE_DECL__SC_NPROCESSORS_CONF)\n      unsigned int max_num = 0;\n      long sc_value;\n      sc_value = sysconf (_SC_NPROCESSORS_ONLN);\n      if (0 < sc_value)\n        max_num = (unsigned int) sc_value;\n      if (0 < max_num)\n      {\n        ret = 0;\n        for (cpu_num = 0; cpu_num < max_num; ++cpu_num)\n          if (0 < cpuset_isset (cpu_num, cpuset_ptr))\n            ++ret;\n      }\n      else /* Combined with the next 'if' */\n#endif /* HAVE_SYSCONF && HAVE_DECL__SC_NPROCESSORS_CONF */\n      if (1)\n      {\n        int res;\n        cpu_num = 0;\n        ret = 0;\n        do\n        {\n          res = cpuset_isset (cpu_num++, cpuset_ptr);\n          if (0 < res)\n            ++ret;\n        } while (0 <= res);\n      }\n```\nFocus on the above code fragment and judge whether it violates the rule 2.2 or not.\n</target_code>\n",
  "is_rule_violated": false,
  "scenario": "No violation. The apparent redundant 'if (1)' is used to form an 'else if' when HAVE_SYSCONF && HAVE_DECL__SC_NPROCESSORS_CONF are defined (the preceding 'else' is provided in the code before the #endif). When those macros are defined the do/while block is executed only if max_num <= 0; when the macros are not defined the 'if (1)' becomes an unconditional branch. Thus the block can execute in some build/config/runtime combinations and is not dead code."
}