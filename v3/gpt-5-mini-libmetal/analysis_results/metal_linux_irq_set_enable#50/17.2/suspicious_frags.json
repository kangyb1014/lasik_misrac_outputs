{
  "user_message": "<misra_rule>\nThe target MISRA C 2023 Rule 17.2 is: Functions shall not call themselves, either directly or indirectly.\n\n</misra_rule>\n<instruction>\nOther agents will judge whether the code follows the MISRA C 2023 guidelines.\nThey will investigate the code fragments you give.\nYour job is to find code fragments in the code that should be investigated to check whether the code follows the MISRA C rule.\n</instruction>\n<response format>\nProvide your answer with these fields.\n- suspicious_frags: The list of suspicious code fragments in the code.\nIt should contain only the code fragments exists in the code. If you do not find any suspicious code fragments, return an empty list for suspicious_frags\n- explanation: The explanation of why other code fragments should NOT be investigated.\nIt is very important to not miss any suspicious code fragments, so please give a sufficient explanation why other code fragments should not be investigated.\nDO NOT give any explanation for the selected suspicious code fragments.\n</response format>\n<target_code>\nHere is the code of metal_linux_irq_set_enable:\n```c\nstatic void metal_linux_irq_set_enable(struct metal_irq_controller *irq_cntr,\n\t\t\t\t       int irq, unsigned int state)\n{\n\tint offset, ret;\n\n\tif (irq < irq_cntr->irq_base ||\n\t    irq >= irq_cntr->irq_base + irq_cntr->irq_num) {\n\t\tmetal_log(METAL_LOG_ERROR, \"%s: invalid irq %d\\n\",\n\t\t\t  __func__, irq);\n\t\treturn;\n\t}\n\toffset = irq - linux_irq_cntr.irq_base;\n\tmetal_mutex_acquire(&irq_lock);\n\tif (state == METAL_IRQ_ENABLE)\n\t\tmetal_bitmap_set_bit(irqs_enabled, offset);\n\telse\n\t\tmetal_bitmap_clear_bit(irqs_enabled, offset);\n\tmetal_mutex_release(&irq_lock);\n\t/* Notify IRQ thread that IRQ state has changed */\n\tret = metal_linux_irq_notify();\n\tif (ret < 0) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to notify set %d enable\\n\",\n\t\t\t  __func__, irq);\n\t}\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are related to the target function metal_linux_irq_set_enable:\nstatic int metal_linux_irq_notify(void)\n{\n\tuint64_t val = 1;\n\tint ret;\n\n\tret = write(irq_notify_fd, &val, sizeof(val));\n\tif (ret < 0) {\n\t\tmetal_log(METAL_LOG_ERROR, \"%s failed\\n\", __func__);\n\t}\n\treturn ret;\n}\n...\nint metal_condition_wait(struct metal_condition *cv,\n\t\t\t\t       metal_mutex_t *m)\n{\n\tuintptr_t tmpmptr = 0, mptr = (uintptr_t)m;\n\tint v = 0;\n\n\t/* Check if the mutex has been acquired */\n\tif (!cv || !m || !metal_mutex_is_acquired(m))\n\t\treturn -EINVAL;\n\n\tif (!atomic_compare_exchange_strong(&cv->mptr, &tmpmptr, mptr)) {\n\t\tif (tmpmptr != mptr)\n\t\t\treturn -EINVAL;\n\t}\n\n\tv = atomic_load(&cv->wakeups);\n\tatomic_fetch_add(&cv->waiters, 1);\n\n\t/* Release the mutex before sleeping. */\n\tmetal_mutex_release(m);\n\tsyscall(SYS_futex, &cv->wakeups, FUTEX_WAIT, v, NULL, NULL, 0);\n\tatomic_fetch_sub(&cv->waiters, 1);\n\t/* Acquire the mutex after it's waken up. */\n\tmetal_mutex_acquire(m);\n\n\treturn 0;\n}\n...\nunsigned int metal_irq_save_disable(void)\n{\n\t/* This is to avoid deadlock if it is called in ISR */\n\tif (pthread_self() == irq_pthread)\n\t\treturn 0;\n\tmetal_mutex_acquire(&irq_lock);\n\treturn 0;\n}\n...\nvoid metal_irq_restore_enable(unsigned int flags)\n{\n\t(void)flags;\n\tif (pthread_self() != irq_pthread)\n\t\tmetal_mutex_release(&irq_lock);\n}\n...\nstatic void *metal_linux_irq_handling(void *args)\n{\n\tstruct sched_param param;\n\tuint64_t val;\n\tint ret;\n\tint i, j, pfds_total;\n\tstruct pollfd *pfds;\n\n\t(void)args;\n\n\tpfds = (struct pollfd *)malloc(FD_SETSIZE * sizeof(struct pollfd));\n\tif (!pfds) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to allocate irq fds mem.\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tparam.sched_priority = sched_get_priority_max(SCHED_FIFO);\n\t/* Ignore the set scheduler error */\n\tret = sched_setscheduler(0, SCHED_FIFO, &param);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_WARNING,\n\t\t\t  \"%s: Failed to set scheduler: %s.\\n\", __func__,\n\t\t\t  strerror(ret));\n\t}\n\n\twhile (1) {\n\t\tmetal_mutex_acquire(&irq_lock);\n\t\tif (irq_handling_stop) {\n\t\t\t/* Killing this IRQ handling thread */\n\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Get the fdset */\n\t\tmemset(pfds, 0, MAX_IRQS * sizeof(struct pollfd));\n\t\tpfds[0].fd = irq_notify_fd;\n\t\tpfds[0].events = POLLIN;\n\t\tj = 1;\n\t\tmetal_bitmap_for_each_set_bit(irqs_enabled, i,\n\t\t\t\t\t      linux_irq_cntr.irq_num) {\n\t\t\tpfds[j].fd = i;\n\t\t\tpfds[j].events = POLLIN;\n\t\t\tj++;\n\t\t}\n\t\tmetal_mutex_release(&irq_lock);\n\t\t/* Wait for interrupt */\n\t\tret = poll(pfds, j, -1);\n\t\tif (ret < 0) {\n\t\t\tmetal_log(METAL_LOG_ERROR, \"%s: poll() failed: %s.\\n\",\n\t\t\t\t  __func__, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\t/* Waken up from interrupt */\n\t\tpfds_total = j;\n\t\tfor (i = 0; i < pfds_total; i++) {\n\t\t\tif ((pfds[i].fd == irq_notify_fd) &&\n\t\t\t    (pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\t/* IRQ registration change notification */\n\t\t\t\tif (read(pfds[i].fd,\n\t\t\t\t\t (void *)&val, sizeof(uint64_t)) < 0)\n\t\t\t\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t\t\t\t  \"%s, read irq fd %d failed\\n\",\n\t\t\t\t\t\t  __func__, pfds[i].fd);\n\t\t\t} else if ((pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\tstruct metal_device *dev = NULL;\n\t\t\t\tint irq_handled = 0;\n\t\t\t\tint fd;\n\n\t\t\t\tfd = pfds[i].fd;\n\t\t\t\tdev = irqs_devs[fd];\n\t\t\t\tmetal_mutex_acquire(&irq_lock);\n\t\t\t\tif (metal_irq_handle(&irqs[fd], fd)\n\t\t\t\t    == METAL_IRQ_HANDLED)\n\t\t\t\t\tirq_handled = 1;\n\t\t\t\tif (irq_handled) {\n\t\t\t\t\tif (dev && dev->bus->ops.dev_irq_ack)\n\t\t\t\t\t\tdev->bus->ops.dev_irq_ack(\n\t\t\t\t\t\t\tdev->bus, dev, fd);\n\t\t\t\t}\n\t\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\t} else if (pfds[i].revents) {\n\t\t\t\tmetal_log(METAL_LOG_DEBUG,\n\t\t\t\t\t  \"%s: poll unexpected. fd %d: %d\\n\",\n\t\t\t\t\t  __func__,\n\t\t\t\t\t  pfds[i].fd, pfds[i].revents);\n\t\t\t}\n\t\t}\n\t}\n\tfree(pfds);\n\treturn NULL;\n}\n...\nstatic void *mutex_thread(void *arg)\n{\n\tmetal_mutex_t *l = arg;\n\tint i;\n\n\tfor (i = 0; i < mutex_test_count; i++) {\n\t\tmetal_mutex_acquire(l);\n\t\tusleep(1);\n\t\tmetal_mutex_release(l);\n\t}\n\n\treturn NULL;\n}\n...\nstatic void *consumer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (!counter)\n\t\tmetal_condition_wait(&nempty_condv, &lock);\n\tcounter--;\n\tmetal_condition_signal(&nfull_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n...\nstatic void *producer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (counter == COUNTER_MAX)\n\t\tmetal_condition_wait(&nfull_condv, &lock);\n\tcounter++;\n\tmetal_condition_signal(&nempty_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n...\nvoid metal_linux_irq_shutdown(void)\n{\n\tint ret;\n\n\tmetal_log(METAL_LOG_DEBUG, \"%s\\n\", __func__);\n\tirq_handling_stop = true;\n\tmetal_linux_irq_notify();\n\tret = pthread_join(irq_pthread, NULL);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_ERROR, \"Failed to join IRQ thread: %d.\\n\",\n\t\t\t  ret);\n\t}\n\tclose(irq_notify_fd);\n\tmetal_mutex_deinit(&irq_lock);\n}\n</additional code>\nPlease find suspicious code fragments in the code and report them. Other agents can not investigate other fragments that you do not report, so it is very important to find all suspicious fragments without missing any. Report the suspicious fragments AS MUCH AS POSSIBLE.",
  "suspicious_frags": [
    "static void *consumer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (!counter)\n\t\tmetal_condition_wait(&nempty_condv, &lock);\n\tcounter--;\n\tmetal_condition_signal(&nfull_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n",
    "static void metal_linux_irq_set_enable(struct metal_irq_controller *irq_cntr,\n\t\t\t\t       int irq, unsigned int state)\n{\n\tint offset, ret;\n\n\tif (irq < irq_cntr->irq_base ||\n\t    irq >= irq_cntr->irq_base + irq_cntr->irq_num) {\n\t\tmetal_log(METAL_LOG_ERROR, \"%s: invalid irq %d\\n\",\n\t\t\t  __func__, irq);\n\t\treturn;\n\t}\n\toffset = irq - linux_irq_cntr.irq_base;\n\tmetal_mutex_acquire(&irq_lock);\n\tif (state == METAL_IRQ_ENABLE)\n\t\tmetal_bitmap_set_bit(irqs_enabled, offset);\n\telse\n\t\tmetal_bitmap_clear_bit(irqs_enabled, offset);\n\tmetal_mutex_release(&irq_lock);\n\t/* Notify IRQ thread that IRQ state has changed */\n\tret = metal_linux_irq_notify();\n\tif (ret < 0) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to notify set %d enable\\n\",\n\t\t\t  __func__, irq);\n\t\t}\n}\n",
    "void metal_linux_irq_shutdown(void)\n{\n\tint ret;\n\n\tmetal_log(METAL_LOG_DEBUG, \"%s\\n\", __func__);\n\tirq_handling_stop = true;\n\tmetal_linux_irq_notify();\n\tret = pthread_join(irq_pthread, NULL);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_ERROR, \"Failed to join IRQ thread: %d.\\n\",\n\t\t\t  ret);\n\t}\n\tclose(irq_notify_fd);\n\tmetal_mutex_deinit(&irq_lock);\n}\n",
    "static void *producer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (counter == COUNTER_MAX)\n\t\tmetal_condition_wait(&nfull_condv, &lock);\n\tcounter++;\n\tmetal_condition_signal(&nempty_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n",
    "static void *metal_linux_irq_handling(void *args)\n{\n\tstruct sched_param param;\n\tuint64_t val;\n\tint ret;\n\tint i, j, pfds_total;\n\tstruct pollfd *pfds;\n\n\t(void)args;\n\n\tpfds = (struct pollfd *)malloc(FD_SETSIZE * sizeof(struct pollfd));\n\tif (!pfds) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to allocate irq fds mem.\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tparam.sched_priority = sched_get_priority_max(SCHED_FIFO);\n\t/* Ignore the set scheduler error */\n\tret = sched_setscheduler(0, SCHED_FIFO, &param);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_WARNING,\n\t\t\t  \"%s: Failed to set scheduler: %s.\\n\", __func__,\n\t\t\t  strerror(ret));\n\t}\n\n\twhile (1) {\n\t\tmetal_mutex_acquire(&irq_lock);\n\t\tif (irq_handling_stop) {\n\t\t\t/* Killing this IRQ handling thread */\n\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Get the fdset */\n\t\tmemset(pfds, 0, MAX_IRQS * sizeof(struct pollfd));\n\t\tpfds[0].fd = irq_notify_fd;\n\t\tpfds[0].events = POLLIN;\n\t\tj = 1;\n\t\tmetal_bitmap_for_each_set_bit(irqs_enabled, i,\n\t\t\t\t\t      linux_irq_cntr.irq_num) {\n\t\t\tpfds[j].fd = i;\n\t\t\tpfds[j].events = POLLIN;\n\t\t\tj++;\n\t\t}\n\t\tmetal_mutex_release(&irq_lock);\n\t\t/* Wait for interrupt */\n\t\tret = poll(pfds, j, -1);\n\t\tif (ret < 0) {\n\t\t\tmetal_log(METAL_LOG_ERROR, \"%s: poll() failed: %s.\\n\",\n\t\t\t\t  __func__, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\t/* Waken up from interrupt */\n\t\tpfds_total = j;\n\t\tfor (i = 0; i < pfds_total; i++) {\n\t\t\tif ((pfds[i].fd == irq_notify_fd) &&\n\t\t\t    (pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\t/* IRQ registration change notification */\n\t\t\t\tif (read(pfds[i].fd,\n\t\t\t\t     (void *)&val, sizeof(uint64_t)) < 0)\n\t\t\t\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t\t\t      \"%s, read irq fd %d failed\\n\",\n\t\t\t\t\t      __func__, pfds[i].fd);\n\t\t\t} else if ((pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\tstruct metal_device *dev = NULL;\n\t\t\t\tint irq_handled = 0;\n\t\t\t\tint fd;\n\n\t\t\t\tfd = pfds[i].fd;\n\t\t\t\tdev = irqs_devs[fd];\n\t\t\t\tmetal_mutex_acquire(&irq_lock);\n\t\t\t\tif (metal_irq_handle(&irqs[fd], fd)\n\t\t\t\t    == METAL_IRQ_HANDLED)\n\t\t\t\t\tirq_handled = 1;\n\t\t\t\tif (irq_handled) {\n\t\t\t\t\tif (dev && dev->bus->ops.dev_irq_ack)\n\t\t\t\t\t\tdev->bus->ops.dev_irq_ack(\n\t\t\t\t\t\t\tdev->bus, dev, fd);\n\t\t\t\t}\n\t\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\t} else if (pfds[i].revents) {\n\t\t\t\tmetal_log(METAL_LOG_DEBUG,\n\t\t\t\t\t  \"%s: poll unexpected. fd %d: %d\\n\",\n\t\t\t\t\t  __func__,\n\t\t\t\t\t  pfds[i].fd, pfds[i].revents);\n\t\t\t}\n\t\t}\n\t}\n\tfree(pfds);\n\treturn NULL;\n}\n",
    "int metal_condition_wait(struct metal_condition *cv,\n\t\t\t       metal_mutex_t *m)\n{\n\tuintptr_t tmpmptr = 0, mptr = (uintptr_t)m;\n\tint v = 0;\n\n\t/* Check if the mutex has been acquired */\n\tif (!cv || !m || !metal_mutex_is_acquired(m))\n\t\treturn -EINVAL;\n\n\tif (!atomic_compare_exchange_strong(&cv->mptr, &tmpmptr, mptr)) {\n\t\tif (tmpmptr != mptr)\n\t\t\treturn -EINVAL;\n\t}\n\n\tv = atomic_load(&cv->wakeups);\n\tatomic_fetch_add(&cv->waiters, 1);\n\n\t/* Release the mutex before sleeping. */\n\tmetal_mutex_release(m);\n\tsyscall(SYS_futex, &cv->wakeups, FUTEX_WAIT, v, NULL, NULL, 0);\n\tatomic_fetch_sub(&cv->waiters, 1);\n\t/* Acquire the mutex after it's waken up. */\n\tmetal_mutex_acquire(m);\n\n\treturn 0;\n}\n",
    "static int metal_linux_irq_notify(void)\n{\n\tuint64_t val = 1;\n\tint ret;\n\n\tret = write(irq_notify_fd, &val, sizeof(val));\n\tif (ret < 0) {\n\t\tmetal_log(METAL_LOG_ERROR, \"%s failed\\n\", __func__);\n\t}\n\treturn ret;\n}\n",
    "static void *mutex_thread(void *arg)\n{\n\tmetal_mutex_t *l = arg;\n\tint i;\n\n\tfor (i = 0; i < mutex_test_count; i++) {\n\t\tmetal_mutex_acquire(l);\n\t\tusleep(1);\n\t\tmetal_mutex_release(l);\n\t}\n\n\treturn NULL;\n}\n",
    "unsigned int metal_irq_save_disable(void)\n{\n\t/* This is to avoid deadlock if it is called in ISR */\n\tif (pthread_self() == irq_pthread)\n\t\treturn 0;\n\tmetal_mutex_acquire(&irq_lock);\n\treturn 0;\n}\n",
    "void metal_irq_restore_enable(unsigned int flags)\n{\n\t(void)flags;\n\tif (pthread_self() != irq_pthread)\n\t\tmetal_mutex_release(&irq_lock);\n}\n"
  ],
  "explanation": "I selected all function fragments that perform calls which could participate in direct or indirect recursion cycles or invoke external callbacks that might call back into IRQ-related APIs. These include: metal_linux_irq_set_enable (calls metal_linux_irq_notify and bitmap/mutex ops), metal_linux_irq_notify (performs write on notify fd), metal_linux_irq_handling (poll loop that calls metal_irq_handle and device callback dev_irq_ack while holding/releasing the irq_lock), the condition/wait helpers and mutex wrappers (metal_condition_wait, metal_irq_save_disable, metal_irq_restore_enable) because they acquire/release mutexes and interact with the IRQ thread identity, and shutdown/worker threads (metal_linux_irq_shutdown, mutex_thread, consumer_thread, producer_thread) because they call notify/join/condition primitives. Any of these could be part of an indirect call chain that leads back to a caller (via callbacks or shared APIs), so they must be investigated.\n\nOther code not listed (e.g., simple atomic operations, local variable handling, memory allocation/free, or straightforward logging calls) do not perform function calls that can plausibly form a recursion cycle in the provided snippets, so they are not included for investigation."
}