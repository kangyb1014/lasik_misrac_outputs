{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 17.2 is: Functions shall not call themselves, either directly or indirectly.\n\n</misra_rule>\n<response format>\nProvide your answer with these fields.\n- is_rule_violated: whether the rule is violated or not (true/false),\n- scenario: a specific violation scenario.\n</response format>\n<target_code>\nHere is the code of metal_condition_wait:\n```c\nint metal_condition_wait(struct metal_condition *cv,\n\t\t\t\t       metal_mutex_t *m)\n{\n\tuintptr_t tmpmptr = 0, mptr = (uintptr_t)m;\n\tint v = 0;\n\n\t/* Check if the mutex has been acquired */\n\tif (!cv || !m || !metal_mutex_is_acquired(m))\n\t\treturn -EINVAL;\n\n\tif (!atomic_compare_exchange_strong(&cv->mptr, &tmpmptr, mptr)) {\n\t\tif (tmpmptr != mptr)\n\t\t\treturn -EINVAL;\n\t}\n\n\tv = atomic_load(&cv->wakeups);\n\tatomic_fetch_add(&cv->waiters, 1);\n\n\t/* Release the mutex before sleeping. */\n\tmetal_mutex_release(m);\n\tsyscall(SYS_futex, &cv->wakeups, FUTEX_WAIT, v, NULL, NULL, 0);\n\tatomic_fetch_sub(&cv->waiters, 1);\n\t/* Acquire the mutex after it's waken up. */\n\tmetal_mutex_acquire(m);\n\n\treturn 0;\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are close to the target function metal_condition_wait:\nunsigned int metal_irq_save_disable(void)\n{\n\t/* This is to avoid deadlock if it is called in ISR */\n\tif (pthread_self() == irq_pthread)\n\t\treturn 0;\n\tmetal_mutex_acquire(&irq_lock);\n\treturn 0;\n}\n...\nvoid metal_irq_restore_enable(unsigned int flags)\n{\n\t(void)flags;\n\tif (pthread_self() != irq_pthread)\n\t\tmetal_mutex_release(&irq_lock);\n}\n...\nstatic void metal_linux_irq_set_enable(struct metal_irq_controller *irq_cntr,\n\t\t\t\t       int irq, unsigned int state)\n{\n\tint offset, ret;\n\n\tif (irq < irq_cntr->irq_base ||\n\t    irq >= irq_cntr->irq_base + irq_cntr->irq_num) {\n\t\tmetal_log(METAL_LOG_ERROR, \"%s: invalid irq %d\\n\",\n\t\t\t  __func__, irq);\n\t\treturn;\n\t}\n\toffset = irq - linux_irq_cntr.irq_base;\n\tmetal_mutex_acquire(&irq_lock);\n\tif (state == METAL_IRQ_ENABLE)\n\t\tmetal_bitmap_set_bit(irqs_enabled, offset);\n\telse\n\t\tmetal_bitmap_clear_bit(irqs_enabled, offset);\n\tmetal_mutex_release(&irq_lock);\n\t/* Notify IRQ thread that IRQ state has changed */\n\tret = metal_linux_irq_notify();\n\tif (ret < 0) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to notify set %d enable\\n\",\n\t\t\t  __func__, irq);\n\t}\n}\n...\nstatic void *metal_linux_irq_handling(void *args)\n{\n\tstruct sched_param param;\n\tuint64_t val;\n\tint ret;\n\tint i, j, pfds_total;\n\tstruct pollfd *pfds;\n\n\t(void)args;\n\n\tpfds = (struct pollfd *)malloc(FD_SETSIZE * sizeof(struct pollfd));\n\tif (!pfds) {\n\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t  \"%s: failed to allocate irq fds mem.\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tparam.sched_priority = sched_get_priority_max(SCHED_FIFO);\n\t/* Ignore the set scheduler error */\n\tret = sched_setscheduler(0, SCHED_FIFO, &param);\n\tif (ret) {\n\t\tmetal_log(METAL_LOG_WARNING,\n\t\t\t  \"%s: Failed to set scheduler: %s.\\n\", __func__,\n\t\t\t  strerror(ret));\n\t}\n\n\twhile (1) {\n\t\tmetal_mutex_acquire(&irq_lock);\n\t\tif (irq_handling_stop) {\n\t\t\t/* Killing this IRQ handling thread */\n\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Get the fdset */\n\t\tmemset(pfds, 0, MAX_IRQS * sizeof(struct pollfd));\n\t\tpfds[0].fd = irq_notify_fd;\n\t\tpfds[0].events = POLLIN;\n\t\tj = 1;\n\t\tmetal_bitmap_for_each_set_bit(irqs_enabled, i,\n\t\t\t\t\t      linux_irq_cntr.irq_num) {\n\t\t\tpfds[j].fd = i;\n\t\t\tpfds[j].events = POLLIN;\n\t\t\tj++;\n\t\t}\n\t\tmetal_mutex_release(&irq_lock);\n\t\t/* Wait for interrupt */\n\t\tret = poll(pfds, j, -1);\n\t\tif (ret < 0) {\n\t\t\tmetal_log(METAL_LOG_ERROR, \"%s: poll() failed: %s.\\n\",\n\t\t\t\t  __func__, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\t/* Waken up from interrupt */\n\t\tpfds_total = j;\n\t\tfor (i = 0; i < pfds_total; i++) {\n\t\t\tif ((pfds[i].fd == irq_notify_fd) &&\n\t\t\t    (pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\t/* IRQ registration change notification */\n\t\t\t\tif (read(pfds[i].fd,\n\t\t\t\t\t (void *)&val, sizeof(uint64_t)) < 0)\n\t\t\t\t\tmetal_log(METAL_LOG_ERROR,\n\t\t\t\t\t\t  \"%s, read irq fd %d failed\\n\",\n\t\t\t\t\t\t  __func__, pfds[i].fd);\n\t\t\t} else if ((pfds[i].revents & (POLLIN | POLLRDNORM))) {\n\t\t\t\tstruct metal_device *dev = NULL;\n\t\t\t\tint irq_handled = 0;\n\t\t\t\tint fd;\n\n\t\t\t\tfd = pfds[i].fd;\n\t\t\t\tdev = irqs_devs[fd];\n\t\t\t\tmetal_mutex_acquire(&irq_lock);\n\t\t\t\tif (metal_irq_handle(&irqs[fd], fd)\n\t\t\t\t    == METAL_IRQ_HANDLED)\n\t\t\t\t\tirq_handled = 1;\n\t\t\t\tif (irq_handled) {\n\t\t\t\t\tif (dev && dev->bus->ops.dev_irq_ack)\n\t\t\t\t\t\tdev->bus->ops.dev_irq_ack(\n\t\t\t\t\t\t\tdev->bus, dev, fd);\n\t\t\t\t}\n\t\t\t\tmetal_mutex_release(&irq_lock);\n\t\t\t} else if (pfds[i].revents) {\n\t\t\t\tmetal_log(METAL_LOG_DEBUG,\n\t\t\t\t\t  \"%s: poll unexpected. fd %d: %d\\n\",\n\t\t\t\t\t  __func__,\n\t\t\t\t\t  pfds[i].fd, pfds[i].revents);\n\t\t\t}\n\t\t}\n\t}\n\tfree(pfds);\n\treturn NULL;\n}\n...\nstatic void *mutex_thread(void *arg)\n{\n\tmetal_mutex_t *l = arg;\n\tint i;\n\n\tfor (i = 0; i < mutex_test_count; i++) {\n\t\tmetal_mutex_acquire(l);\n\t\tusleep(1);\n\t\tmetal_mutex_release(l);\n\t}\n\n\treturn NULL;\n}\n...\nstatic void *consumer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (!counter)\n\t\tmetal_condition_wait(&nempty_condv, &lock);\n\tcounter--;\n\tmetal_condition_signal(&nfull_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n...\nstatic void *producer_thread(void *arg)\n{\n\t(void)arg;\n\tmetal_mutex_acquire(&lock);\n\twhile (counter == COUNTER_MAX)\n\t\tmetal_condition_wait(&nfull_condv, &lock);\n\tcounter++;\n\tmetal_condition_signal(&nempty_condv);\n\tmetal_mutex_release(&lock);\n\n\treturn NULL;\n}\n</additional code>\n<target_code>\nHere is the code fragment that you need to analyze:\n```c\nvoid metal_irq_restore_enable(unsigned int flags)\n{\n\t(void)flags;\n\tif (pthread_self() != irq_pthread)\n\t\tmetal_mutex_release(&irq_lock);\n}\n\n```\nFocus on the above code fragment and judge whether it violates the rule 17.2 or not.\n</target_code>\n",
  "is_rule_violated": false,
  "scenario": "No violation: metal_irq_restore_enable does not call itself directly and there is no evidence of indirect recursion. It only calls metal_mutex_release(&irq_lock) when pthread_self() != irq_pthread; metal_mutex_release (and other shown functions like metal_irq_save_disable calling metal_mutex_acquire) do not call metal_irq_restore_enable, so neither direct nor indirect self-calls occur."
}