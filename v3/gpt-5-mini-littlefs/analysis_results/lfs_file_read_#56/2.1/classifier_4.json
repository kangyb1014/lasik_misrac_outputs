{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 2.1 is: A project shall not contain unreachable code.\n\n</misra_rule>\n<response format>\nProvide your answer with these fields.\n- is_rule_violated: whether the rule is violated or not (true/false),\n- scenario: a specific violation scenario.\n</response format>\n<target_code>\nHere is the code of lfs_file_read_:\n```c\nstatic lfs_ssize_t lfs_file_read_(lfs_t *lfs, lfs_file_t *file,\n        void *buffer, lfs_size_t size) {\n    LFS_ASSERT((file->flags & LFS_O_RDONLY) == LFS_O_RDONLY);\n\n#ifndef LFS_READONLY\n    if (file->flags & LFS_F_WRITING) {\n        // flush out any writes\n        int err = lfs_file_flush(lfs, file);\n        if (err) {\n            return err;\n        }\n    }\n#endif\n\n    return lfs_file_flushedread(lfs, file, buffer, size);\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are close to the target function lfs_file_read_:\n#define LFS_ASSERT(test) assert(test)\n...\nint lfs_file_open(lfs_t *lfs, lfs_file_t *file, const char *path, int flags) {\n    int err = LFS_LOCK(lfs->cfg);\n    if (err) {\n        return err;\n    }\n    LFS_TRACE(\"lfs_file_open(%p, %p, \\\"%s\\\", %x)\",\n            (void*)lfs, (void*)file, path, (unsigned)flags);\n    LFS_ASSERT(!lfs_mlist_isopen(lfs->mlist, (struct lfs_mlist*)file));\n\n    err = lfs_file_open_(lfs, file, path, flags);\n\n    LFS_TRACE(\"lfs_file_open -> %d\", err);\n    LFS_UNLOCK(lfs->cfg);\n    return err;\n}\n...\nstatic lfs_ssize_t lfs_file_write_(lfs_t *lfs, lfs_file_t *file,\n        const void *buffer, lfs_size_t size) {\n    LFS_ASSERT((file->flags & LFS_O_WRONLY) == LFS_O_WRONLY);\n\n    if (file->flags & LFS_F_READING) {\n        // drop any reads\n        int err = lfs_file_flush(lfs, file);\n        if (err) {\n            return err;\n        }\n    }\n\n    if ((file->flags & LFS_O_APPEND) && file->pos < file->ctz.size) {\n        file->pos = file->ctz.size;\n    }\n\n    if (file->pos + size > lfs->file_max) {\n        // Larger than file limit?\n        return LFS_ERR_FBIG;\n    }\n\n    if (!(file->flags & LFS_F_WRITING) && file->pos > file->ctz.size) {\n        // fill with zeros\n        lfs_off_t pos = file->pos;\n        file->pos = file->ctz.size;\n\n        while (file->pos < pos) {\n            lfs_ssize_t res = lfs_file_flushedwrite(lfs, file, &(uint8_t){0}, 1);\n            if (res < 0) {\n                return res;\n            }\n        }\n    }\n\n    lfs_ssize_t nsize = lfs_file_flushedwrite(lfs, file, buffer, size);\n    if (nsize < 0) {\n        return nsize;\n    }\n\n    file->flags &= ~LFS_F_ERRED;\n    return nsize;\n}\n...\nstatic int lfs_file_flush(lfs_t *lfs, lfs_file_t *file) {\n    if (file->flags & LFS_F_READING) {\n        if (!(file->flags & LFS_F_INLINE)) {\n            lfs_cache_drop(lfs, &file->cache);\n        }\n        file->flags &= ~LFS_F_READING;\n    }\n\n#ifndef LFS_READONLY\n    if (file->flags & LFS_F_WRITING) {\n        lfs_off_t pos = file->pos;\n\n        if (!(file->flags & LFS_F_INLINE)) {\n            // copy over anything after current branch\n            lfs_file_t orig = {\n                .ctz.head = file->ctz.head,\n                .ctz.size = file->ctz.size,\n                .flags = LFS_O_RDONLY,\n                .pos = file->pos,\n                .cache = lfs->rcache,\n            };\n            lfs_cache_drop(lfs, &lfs->rcache);\n\n            while (file->pos < file->ctz.size) {\n                // copy over a byte at a time, leave it up to caching\n                // to make this efficient\n                uint8_t data;\n                lfs_ssize_t res = lfs_file_flushedread(lfs, &orig, &data, 1);\n                if (res < 0) {\n                    return res;\n                }\n\n                res = lfs_file_flushedwrite(lfs, file, &data, 1);\n                if (res < 0) {\n                    return res;\n                }\n\n                // keep our reference to the rcache in sync\n                if (lfs->rcache.block != LFS_BLOCK_NULL) {\n                    lfs_cache_drop(lfs, &orig.cache);\n                    lfs_cache_drop(lfs, &lfs->rcache);\n                }\n            }\n\n            // write out what we have\n            while (true) {\n                int err = lfs_bd_flush(lfs, &file->cache, &lfs->rcache, true);\n                if (err) {\n                    if (err == LFS_ERR_CORRUPT) {\n                        goto relocate;\n                    }\n                    return err;\n                }\n\n                break;\n\nrelocate:\n                LFS_DEBUG(\"Bad block at 0x%\"PRIx32, file->block);\n                err = lfs_file_relocate(lfs, file);\n                if (err) {\n                    return err;\n                }\n            }\n        } else {\n            file->pos = lfs_max(file->pos, file->ctz.size);\n        }\n\n        // actual file updates\n        file->ctz.head = file->block;\n        file->ctz.size = file->pos;\n        file->flags &= ~LFS_F_WRITING;\n        file->flags |= LFS_F_DIRTY;\n\n        file->pos = pos;\n    }\n#endif\n\n    return 0;\n}\n...\nstatic int lfs_file_sync_(lfs_t *lfs, lfs_file_t *file) {\n    if (file->flags & LFS_F_ERRED) {\n        // it's not safe to do anything if our file errored\n        return 0;\n    }\n\n    int err = lfs_file_flush(lfs, file);\n    if (err) {\n        file->flags |= LFS_F_ERRED;\n        return err;\n    }\n\n\n    if ((file->flags & LFS_F_DIRTY) &&\n            !lfs_pair_isnull(file->m.pair)) {\n        // before we commit metadata, we need sync the disk to make sure\n        // data writes don't complete after metadata writes\n        if (!(file->flags & LFS_F_INLINE)) {\n            err = lfs_bd_sync(lfs, &lfs->pcache, &lfs->rcache, false);\n            if (err) {\n                return err;\n            }\n        }\n\n        // update dir entry\n        uint16_t type;\n        const void *buffer;\n        lfs_size_t size;\n        struct lfs_ctz ctz;\n        if (file->flags & LFS_F_INLINE) {\n            // inline the whole file\n            type = LFS_TYPE_INLINESTRUCT;\n            buffer = file->cache.buffer;\n            size = file->ctz.size;\n        } else {\n            // update the ctz reference\n            type = LFS_TYPE_CTZSTRUCT;\n            // copy ctz so alloc will work during a relocate\n            ctz = file->ctz;\n            lfs_ctz_tole32(&ctz);\n            buffer = &ctz;\n            size = sizeof(ctz);\n        }\n\n        // commit file data and attributes\n        err = lfs_dir_commit(lfs, &file->m, LFS_MKATTRS(\n                {LFS_MKTAG(type, file->id, size), buffer},\n                {LFS_MKTAG(LFS_FROM_USERATTRS, file->id,\n                    file->cfg->attr_count), file->cfg->attrs}));\n        if (err) {\n            file->flags |= LFS_F_ERRED;\n            return err;\n        }\n\n        file->flags &= ~LFS_F_DIRTY;\n    }\n\n    return 0;\n}\n...\nstatic lfs_soff_t lfs_file_seek_(lfs_t *lfs, lfs_file_t *file,\n        lfs_soff_t off, int whence) {\n    // find new pos\n    //\n    // fortunately for us, littlefs is limited to 31-bit file sizes, so we\n    // don't have to worry too much about integer overflow\n    lfs_off_t npos = file->pos;\n    if (whence == LFS_SEEK_SET) {\n        npos = off;\n    } else if (whence == LFS_SEEK_CUR) {\n        npos = file->pos + (lfs_off_t)off;\n    } else if (whence == LFS_SEEK_END) {\n        npos = (lfs_off_t)lfs_file_size_(lfs, file) + (lfs_off_t)off;\n    }\n\n    if (npos > lfs->file_max) {\n        // file position out of range\n        return LFS_ERR_INVAL;\n    }\n\n    if (file->pos == npos) {\n        // noop - position has not changed\n        return npos;\n    }\n\n    // if we're only reading and our new offset is still in the file's cache\n    // we can avoid flushing and needing to reread the data\n    if ((file->flags & LFS_F_READING)\n            && file->off != lfs->cfg->block_size) {\n        int oindex = lfs_ctz_index(lfs, &(lfs_off_t){file->pos});\n        lfs_off_t noff = npos;\n        int nindex = lfs_ctz_index(lfs, &noff);\n        if (oindex == nindex\n                && noff >= file->cache.off\n                && noff < file->cache.off + file->cache.size) {\n            file->pos = npos;\n            file->off = noff;\n            return npos;\n        }\n    }\n\n    // write out everything beforehand, may be noop if rdonly\n    int err = lfs_file_flush(lfs, file);\n    if (err) {\n        return err;\n    }\n\n    // update pos\n    file->pos = npos;\n    return npos;\n}\n...\nstatic int lfs_file_truncate_(lfs_t *lfs, lfs_file_t *file, lfs_off_t size) {\n    LFS_ASSERT((file->flags & LFS_O_WRONLY) == LFS_O_WRONLY);\n\n    if (size > LFS_FILE_MAX) {\n        return LFS_ERR_INVAL;\n    }\n\n    lfs_off_t pos = file->pos;\n    lfs_off_t oldsize = lfs_file_size_(lfs, file);\n    if (size < oldsize) {\n        // revert to inline file?\n        if (size <= lfs->inline_max) {\n            // flush+seek to head\n            lfs_soff_t res = lfs_file_seek_(lfs, file, 0, LFS_SEEK_SET);\n            if (res < 0) {\n                return (int)res;\n            }\n\n            // read our data into rcache temporarily\n            lfs_cache_drop(lfs, &lfs->rcache);\n            res = lfs_file_flushedread(lfs, file,\n                    lfs->rcache.buffer, size);\n            if (res < 0) {\n                return (int)res;\n            }\n\n            file->ctz.head = LFS_BLOCK_INLINE;\n            file->ctz.size = size;\n            file->flags |= LFS_F_DIRTY | LFS_F_READING | LFS_F_INLINE;\n            file->cache.block = file->ctz.head;\n            file->cache.off = 0;\n            file->cache.size = lfs->cfg->cache_size;\n            memcpy(file->cache.buffer, lfs->rcache.buffer, size);\n\n        } else {\n            // need to flush since directly changing metadata\n            int err = lfs_file_flush(lfs, file);\n            if (err) {\n                return err;\n            }\n\n            // lookup new head in ctz skip list\n            err = lfs_ctz_find(lfs, NULL, &file->cache,\n                    file->ctz.head, file->ctz.size,\n                    size-1, &file->block, &(lfs_off_t){0});\n            if (err) {\n                return err;\n            }\n\n            // need to set pos/block/off consistently so seeking back to\n            // the old position does not get confused\n            file->pos = size;\n            file->ctz.head = file->block;\n            file->ctz.size = size;\n            file->flags |= LFS_F_DIRTY | LFS_F_READING;\n        }\n    } else if (size > oldsize) {\n        // flush+seek if not already at end\n        lfs_soff_t res = lfs_file_seek_(lfs, file, 0, LFS_SEEK_END);\n        if (res < 0) {\n            return (int)res;\n        }\n\n        // fill with zeros\n        while (file->pos < size) {\n            res = lfs_file_write_(lfs, file, &(uint8_t){0}, 1);\n            if (res < 0) {\n                return (int)res;\n            }\n        }\n    }\n\n    // restore pos\n    lfs_soff_t res = lfs_file_seek_(lfs, file, pos, LFS_SEEK_SET);\n    if (res < 0) {\n      return (int)res;\n    }\n\n    return 0;\n}\n</additional code>\n<target_code>\nHere is the code fragment that you need to analyze:\n```c\nLFS_ASSERT((file->flags & LFS_O_WRONLY) == LFS_O_WRONLY);\n```\nFocus on the above code fragment and judge whether it violates the rule 2.1 or not.\n</target_code>\n",
  "is_rule_violated": false,
  "scenario": "No violation. The statement LFS_ASSERT((file->flags & LFS_O_WRONLY) == LFS_O_WRONLY); expands to assert(...) (or to ((void)0) when NDEBUG is defined). This does not create unreachable code: when assertions are enabled the check is evaluated at runtime (and may abort on failure), and when disabled the assertion is removed but does not render subsequent code unreachable. The expression depends on the runtime value of file->flags, so it is not statically unreachable."
}