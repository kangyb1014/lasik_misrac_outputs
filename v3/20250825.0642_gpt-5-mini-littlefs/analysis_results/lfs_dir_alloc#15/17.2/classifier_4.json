{
  "user_message": "<misra_rule>\nMISRA C 2023 Rule 17.2 is: Functions shall not call themselves, either directly or indirectly.\n\n</misra_rule>\n<response format>\nProvide your answer with these fields.\n- is_rule_violated: whether the rule is violated or not (true/false),\n- scenario: a specific violation scenario.\n</response format>\n<target_code>\nHere is the code of lfs_dir_alloc:\n```c\nstatic int lfs_dir_alloc(lfs_t *lfs, lfs_mdir_t *dir) {\n    // allocate pair of dir blocks (backwards, so we write block 1 first)\n    for (int i = 0; i < 2; i++) {\n        int err = lfs_alloc(lfs, &dir->pair[(i+1)%2]);\n        if (err) {\n            return err;\n        }\n    }\n\n    // zero for reproducibility in case initial block is unreadable\n    dir->rev = 0;\n\n    // rather than clobbering one of the blocks we just pretend\n    // the revision may be valid\n    int err = lfs_bd_read(lfs,\n            NULL, &lfs->rcache, sizeof(dir->rev),\n            dir->pair[0], 0, &dir->rev, sizeof(dir->rev));\n    dir->rev = lfs_fromle32(dir->rev);\n    if (err && err != LFS_ERR_CORRUPT) {\n        return err;\n    }\n\n    // to make sure we don't immediately evict, align the new revision count\n    // to our block_cycles modulus, see lfs_dir_compact for why our modulus\n    // is tweaked this way\n    if (lfs->cfg->block_cycles > 0) {\n        dir->rev = lfs_alignup(dir->rev, ((lfs->cfg->block_cycles+1)|1));\n    }\n\n    // set defaults\n    dir->off = sizeof(dir->rev);\n    dir->etag = 0xffffffff;\n    dir->count = 0;\n    dir->tail[0] = LFS_BLOCK_NULL;\n    dir->tail[1] = LFS_BLOCK_NULL;\n    dir->erased = false;\n    dir->split = false;\n\n    // don't write out yet, let caller take care of that\n    return 0;\n}\n```\n</target_code>\n<additional code>Here are the definitions of symbols that are close to the target function lfs_dir_alloc:\nstatic int lfs_alloc_scan(lfs_t *lfs) {\n    // move lookahead buffer to the first unused block\n    //\n    // note we limit the lookahead buffer to at most the amount of blocks\n    // checkpointed, this prevents the math in lfs_alloc from underflowing\n    lfs->lookahead.start = (lfs->lookahead.start + lfs->lookahead.next) \n            % lfs->block_count;\n    lfs->lookahead.next = 0;\n    lfs->lookahead.size = lfs_min(\n            8*lfs->cfg->lookahead_size,\n            lfs->lookahead.ckpoint);\n\n    // find mask of free blocks from tree\n    memset(lfs->lookahead.buffer, 0, lfs->cfg->lookahead_size);\n    int err = lfs_fs_traverse_(lfs, lfs_alloc_lookahead, lfs, true);\n    if (err) {\n        lfs_alloc_drop(lfs);\n        return err;\n    }\n\n    return 0;\n}\n...\nstatic int lfs_dir_split(lfs_t *lfs,\n        lfs_mdir_t *dir, const struct lfs_mattr *attrs, int attrcount,\n        lfs_mdir_t *source, uint16_t split, uint16_t end) {\n    // create tail metadata pair\n    lfs_mdir_t tail;\n    int err = lfs_dir_alloc(lfs, &tail);\n    if (err) {\n        return err;\n    }\n\n    tail.split = dir->split;\n    tail.tail[0] = dir->tail[0];\n    tail.tail[1] = dir->tail[1];\n\n    // note we don't care about LFS_OK_RELOCATED\n    int res = lfs_dir_compact(lfs, &tail, attrs, attrcount, source, split, end);\n    if (res < 0) {\n        return res;\n    }\n\n    dir->tail[0] = tail.pair[0];\n    dir->tail[1] = tail.pair[1];\n    dir->split = true;\n\n    // update root if needed\n    if (lfs_pair_cmp(dir->pair, lfs->root) == 0 && split == 0) {\n        lfs->root[0] = tail.pair[0];\n        lfs->root[1] = tail.pair[1];\n    }\n\n    return 0;\n}\n...\nstatic int lfs_mkdir_(lfs_t *lfs, const char *path) {\n    // deorphan if we haven't yet, needed at most once after poweron\n    int err = lfs_fs_forceconsistency(lfs);\n    if (err) {\n        return err;\n    }\n\n    struct lfs_mlist cwd;\n    cwd.next = lfs->mlist;\n    uint16_t id;\n    err = lfs_dir_find(lfs, &cwd.m, &path, &id);\n    if (!(err == LFS_ERR_NOENT && lfs_path_islast(path))) {\n        return (err < 0) ? err : LFS_ERR_EXIST;\n    }\n\n    // check that name fits\n    lfs_size_t nlen = lfs_path_namelen(path);\n    if (nlen > lfs->name_max) {\n        return LFS_ERR_NAMETOOLONG;\n    }\n\n    // build up new directory\n    lfs_alloc_ckpoint(lfs);\n    lfs_mdir_t dir;\n    err = lfs_dir_alloc(lfs, &dir);\n    if (err) {\n        return err;\n    }\n\n    // find end of list\n    lfs_mdir_t pred = cwd.m;\n    while (pred.split) {\n        err = lfs_dir_fetch(lfs, &pred, pred.tail);\n        if (err) {\n            return err;\n        }\n    }\n\n    // setup dir\n    lfs_pair_tole32(pred.tail);\n    err = lfs_dir_commit(lfs, &dir, LFS_MKATTRS(\n            {LFS_MKTAG(LFS_TYPE_SOFTTAIL, 0x3ff, 8), pred.tail}));\n    lfs_pair_fromle32(pred.tail);\n    if (err) {\n        return err;\n    }\n\n    // current block not end of list?\n    if (cwd.m.split) {\n        // update tails, this creates a desync\n        err = lfs_fs_preporphans(lfs, +1);\n        if (err) {\n            return err;\n        }\n\n        // it's possible our predecessor has to be relocated, and if\n        // our parent is our predecessor's predecessor, this could have\n        // caused our parent to go out of date, fortunately we can hook\n        // ourselves into littlefs to catch this\n        cwd.type = 0;\n        cwd.id = 0;\n        lfs->mlist = &cwd;\n\n        lfs_pair_tole32(dir.pair);\n        err = lfs_dir_commit(lfs, &pred, LFS_MKATTRS(\n                {LFS_MKTAG(LFS_TYPE_SOFTTAIL, 0x3ff, 8), dir.pair}));\n        lfs_pair_fromle32(dir.pair);\n        if (err) {\n            lfs->mlist = cwd.next;\n            return err;\n        }\n\n        lfs->mlist = cwd.next;\n        err = lfs_fs_preporphans(lfs, -1);\n        if (err) {\n            return err;\n        }\n    }\n\n    // now insert into our parent block\n    lfs_pair_tole32(dir.pair);\n    err = lfs_dir_commit(lfs, &cwd.m, LFS_MKATTRS(\n            {LFS_MKTAG(LFS_TYPE_CREATE, id, 0), NULL},\n            {LFS_MKTAG(LFS_TYPE_DIR, id, nlen), path},\n            {LFS_MKTAG(LFS_TYPE_DIRSTRUCT, id, 8), dir.pair},\n            {LFS_MKTAG_IF(!cwd.m.split,\n                LFS_TYPE_SOFTTAIL, 0x3ff, 8), dir.pair}));\n    lfs_pair_fromle32(dir.pair);\n    if (err) {\n        return err;\n    }\n\n    return 0;\n}\n...\nstatic int lfs_format_(lfs_t *lfs, const struct lfs_config *cfg) {\n    int err = 0;\n    {\n        err = lfs_init(lfs, cfg);\n        if (err) {\n            return err;\n        }\n\n        LFS_ASSERT(cfg->block_count != 0);\n\n        // create free lookahead\n        memset(lfs->lookahead.buffer, 0, lfs->cfg->lookahead_size);\n        lfs->lookahead.start = 0;\n        lfs->lookahead.size = lfs_min(8*lfs->cfg->lookahead_size,\n                lfs->block_count);\n        lfs->lookahead.next = 0;\n        lfs_alloc_ckpoint(lfs);\n\n        // create root dir\n        lfs_mdir_t root;\n        err = lfs_dir_alloc(lfs, &root);\n        if (err) {\n            goto cleanup;\n        }\n\n        // write one superblock\n        lfs_superblock_t superblock = {\n            .version     = lfs_fs_disk_version(lfs),\n            .block_size  = lfs->cfg->block_size,\n            .block_count = lfs->block_count,\n            .name_max    = lfs->name_max,\n            .file_max    = lfs->file_max,\n            .attr_max    = lfs->attr_max,\n        };\n\n        lfs_superblock_tole32(&superblock);\n        err = lfs_dir_commit(lfs, &root, LFS_MKATTRS(\n                {LFS_MKTAG(LFS_TYPE_CREATE, 0, 0), NULL},\n                {LFS_MKTAG(LFS_TYPE_SUPERBLOCK, 0, 8), \"littlefs\"},\n                {LFS_MKTAG(LFS_TYPE_INLINESTRUCT, 0, sizeof(superblock)),\n                    &superblock}));\n        if (err) {\n            goto cleanup;\n        }\n\n        // force compaction to prevent accidentally mounting any\n        // older version of littlefs that may live on disk\n        root.erased = false;\n        err = lfs_dir_commit(lfs, &root, NULL, 0);\n        if (err) {\n            goto cleanup;\n        }\n\n        // sanity check that fetch works\n        err = lfs_dir_fetch(lfs, &root, (const lfs_block_t[2]){0, 1});\n        if (err) {\n            goto cleanup;\n        }\n    }\n\ncleanup:\n    lfs_deinit(lfs);\n    return err;\n\n}\n...\nstatic int lfs_dir_compact(lfs_t *lfs,\n        lfs_mdir_t *dir, const struct lfs_mattr *attrs, int attrcount,\n        lfs_mdir_t *source, uint16_t begin, uint16_t end) {\n    // save some state in case block is bad\n    bool relocated = false;\n    bool tired = lfs_dir_needsrelocation(lfs, dir);\n\n    // increment revision count\n    dir->rev += 1;\n\n    // do not proactively relocate blocks during migrations, this\n    // can cause a number of failure states such: clobbering the\n    // v1 superblock if we relocate root, and invalidating directory\n    // pointers if we relocate the head of a directory. On top of\n    // this, relocations increase the overall complexity of\n    // lfs_migration, which is already a delicate operation.\n#ifdef LFS_MIGRATE\n    if (lfs->lfs1) {\n        tired = false;\n    }\n#endif\n\n    if (tired && lfs_pair_cmp(dir->pair, (const lfs_block_t[2]){0, 1}) != 0) {\n        // we're writing too much, time to relocate\n        goto relocate;\n    }\n\n    // begin loop to commit compaction to blocks until a compact sticks\n    while (true) {\n        {\n            // setup commit state\n            struct lfs_commit commit = {\n                .block = dir->pair[1],\n                .off = 0,\n                .ptag = 0xffffffff,\n                .crc = 0xffffffff,\n\n                .begin = 0,\n                .end = (lfs->cfg->metadata_max ?\n                    lfs->cfg->metadata_max : lfs->cfg->block_size) - 8,\n            };\n\n            // erase block to write to\n            int err = lfs_bd_erase(lfs, dir->pair[1]);\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n\n            // write out header\n            dir->rev = lfs_tole32(dir->rev);\n            err = lfs_dir_commitprog(lfs, &commit,\n                    &dir->rev, sizeof(dir->rev));\n            dir->rev = lfs_fromle32(dir->rev);\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n\n            // traverse the directory, this time writing out all unique tags\n            err = lfs_dir_traverse(lfs,\n                    source, 0, 0xffffffff, attrs, attrcount,\n                    LFS_MKTAG(0x400, 0x3ff, 0),\n                    LFS_MKTAG(LFS_TYPE_NAME, 0, 0),\n                    begin, end, -begin,\n                    lfs_dir_commit_commit, &(struct lfs_dir_commit_commit){\n                        lfs, &commit});\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n\n            // commit tail, which may be new after last size check\n            if (!lfs_pair_isnull(dir->tail)) {\n                lfs_pair_tole32(dir->tail);\n                err = lfs_dir_commitattr(lfs, &commit,\n                        LFS_MKTAG(LFS_TYPE_TAIL + dir->split, 0x3ff, 8),\n                        dir->tail);\n                lfs_pair_fromle32(dir->tail);\n                if (err) {\n                    if (err == LFS_ERR_CORRUPT) {\n                        goto relocate;\n                    }\n                    return err;\n                }\n            }\n\n            // bring over gstate?\n            lfs_gstate_t delta = {0};\n            if (!relocated) {\n                lfs_gstate_xor(&delta, &lfs->gdisk);\n                lfs_gstate_xor(&delta, &lfs->gstate);\n            }\n            lfs_gstate_xor(&delta, &lfs->gdelta);\n            delta.tag &= ~LFS_MKTAG(0, 0, 0x3ff);\n\n            err = lfs_dir_getgstate(lfs, dir, &delta);\n            if (err) {\n                return err;\n            }\n\n            if (!lfs_gstate_iszero(&delta)) {\n                lfs_gstate_tole32(&delta);\n                err = lfs_dir_commitattr(lfs, &commit,\n                        LFS_MKTAG(LFS_TYPE_MOVESTATE, 0x3ff,\n                            sizeof(delta)), &delta);\n                if (err) {\n                    if (err == LFS_ERR_CORRUPT) {\n                        goto relocate;\n                    }\n                    return err;\n                }\n            }\n\n            // complete commit with crc\n            err = lfs_dir_commitcrc(lfs, &commit);\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n\n            // successful compaction, swap dir pair to indicate most recent\n            LFS_ASSERT(commit.off % lfs->cfg->prog_size == 0);\n            lfs_pair_swap(dir->pair);\n            dir->count = end - begin;\n            dir->off = commit.off;\n            dir->etag = commit.ptag;\n            // update gstate\n            lfs->gdelta = (lfs_gstate_t){0};\n            if (!relocated) {\n                lfs->gdisk = lfs->gstate;\n            }\n        }\n        break;\n\nrelocate:\n        // commit was corrupted, drop caches and prepare to relocate block\n        relocated = true;\n        lfs_cache_drop(lfs, &lfs->pcache);\n        if (!tired) {\n            LFS_DEBUG(\"Bad block at 0x%\"PRIx32, dir->pair[1]);\n        }\n\n        // can't relocate superblock, filesystem is now frozen\n        if (lfs_pair_cmp(dir->pair, (const lfs_block_t[2]){0, 1}) == 0) {\n            LFS_WARN(\"Superblock 0x%\"PRIx32\" has become unwritable\",\n                    dir->pair[1]);\n            return LFS_ERR_NOSPC;\n        }\n\n        // relocate half of pair\n        int err = lfs_alloc(lfs, &dir->pair[1]);\n        if (err && (err != LFS_ERR_NOSPC || !tired)) {\n            return err;\n        }\n\n        tired = false;\n        continue;\n    }\n\n    return relocated ? LFS_OK_RELOCATED : 0;\n}\n...\nstatic inline int lfs_pair_cmp(\n        const lfs_block_t paira[2],\n        const lfs_block_t pairb[2]) {\n    return !(paira[0] == pairb[0] || paira[1] == pairb[1] ||\n             paira[0] == pairb[1] || paira[1] == pairb[0]);\n}\n...\nstatic int lfs_dir_splittingcompact(lfs_t *lfs, lfs_mdir_t *dir,\n        const struct lfs_mattr *attrs, int attrcount,\n        lfs_mdir_t *source, uint16_t begin, uint16_t end) {\n    while (true) {\n        // find size of first split, we do this by halving the split until\n        // the metadata is guaranteed to fit\n        //\n        // Note that this isn't a true binary search, we never increase the\n        // split size. This may result in poorly distributed metadata but isn't\n        // worth the extra code size or performance hit to fix.\n        lfs_size_t split = begin;\n        while (end - split > 1) {\n            lfs_size_t size = 0;\n            int err = lfs_dir_traverse(lfs,\n                    source, 0, 0xffffffff, attrs, attrcount,\n                    LFS_MKTAG(0x400, 0x3ff, 0),\n                    LFS_MKTAG(LFS_TYPE_NAME, 0, 0),\n                    split, end, -split,\n                    lfs_dir_commit_size, &size);\n            if (err) {\n                return err;\n            }\n\n            // space is complicated, we need room for:\n            //\n            // - tail:         4+2*4 = 12 bytes\n            // - gstate:       4+3*4 = 16 bytes\n            // - move delete:  4     = 4 bytes\n            // - crc:          4+4   = 8 bytes\n            //                 total = 40 bytes\n            //\n            // And we cap at half a block to avoid degenerate cases with\n            // nearly-full metadata blocks.\n            //\n            lfs_size_t metadata_max = (lfs->cfg->metadata_max)\n                    ? lfs->cfg->metadata_max\n                    : lfs->cfg->block_size;\n            if (end - split < 0xff\n                    && size <= lfs_min(\n                        metadata_max - 40,\n                        lfs_alignup(\n                            metadata_max/2,\n                            lfs->cfg->prog_size))) {\n                break;\n            }\n\n            split = split + ((end - split) / 2);\n        }\n\n        if (split == begin) {\n            // no split needed\n            break;\n        }\n\n        // split into two metadata pairs and continue\n        int err = lfs_dir_split(lfs, dir, attrs, attrcount,\n                source, split, end);\n        if (err && err != LFS_ERR_NOSPC) {\n            return err;\n        }\n\n        if (err) {\n            // we can't allocate a new block, try to compact with degraded\n            // performance\n            LFS_WARN(\"Unable to split {0x%\"PRIx32\", 0x%\"PRIx32\"}\",\n                    dir->pair[0], dir->pair[1]);\n            break;\n        } else {\n            end = split;\n        }\n    }\n\n    if (lfs_dir_needsrelocation(lfs, dir)\n            && lfs_pair_cmp(dir->pair, (const lfs_block_t[2]){0, 1}) == 0) {\n        // oh no! we're writing too much to the superblock,\n        // should we expand?\n        lfs_ssize_t size = lfs_fs_size_(lfs);\n        if (size < 0) {\n            return size;\n        }\n\n        // littlefs cannot reclaim expanded superblocks, so expand cautiously\n        //\n        // if our filesystem is more than ~88% full, don't expand, this is\n        // somewhat arbitrary\n        if (lfs->block_count - size > lfs->block_count/8) {\n            LFS_DEBUG(\"Expanding superblock at rev %\"PRIu32, dir->rev);\n            int err = lfs_dir_split(lfs, dir, attrs, attrcount,\n                    source, begin, end);\n            if (err && err != LFS_ERR_NOSPC) {\n                return err;\n            }\n\n            if (err) {\n                // welp, we tried, if we ran out of space there's not much\n                // we can do, we'll error later if we've become frozen\n                LFS_WARN(\"Unable to expand superblock\");\n            } else {\n                // duplicate the superblock entry into the new superblock\n                end = 1;\n            }\n        }\n    }\n\n    return lfs_dir_compact(lfs, dir, attrs, attrcount, source, begin, end);\n}\n...\nstatic int lfs_fs_forceconsistency(lfs_t *lfs) {\n    int err = lfs_fs_desuperblock(lfs);\n    if (err) {\n        return err;\n    }\n\n    err = lfs_fs_demove(lfs);\n    if (err) {\n        return err;\n    }\n\n    err = lfs_fs_deorphan(lfs, true);\n    if (err) {\n        return err;\n    }\n\n    return 0;\n}\n...\nstatic lfs_stag_t lfs_dir_find(lfs_t *lfs, lfs_mdir_t *dir,\n        const char **path, uint16_t *id) {\n    // we reduce path to a single name if we can find it\n    const char *name = *path;\n\n    // default to root dir\n    lfs_stag_t tag = LFS_MKTAG(LFS_TYPE_DIR, 0x3ff, 0);\n    dir->tail[0] = lfs->root[0];\n    dir->tail[1] = lfs->root[1];\n\n    // empty paths are not allowed\n    if (*name == '\\0') {\n        return LFS_ERR_INVAL;\n    }\n\n    while (true) {\nnextname:\n        // skip slashes if we're a directory\n        if (lfs_tag_type3(tag) == LFS_TYPE_DIR) {\n            name += strspn(name, \"/\");\n        }\n        lfs_size_t namelen = strcspn(name, \"/\");\n\n        // skip '.'\n        if (namelen == 1 && memcmp(name, \".\", 1) == 0) {\n            name += namelen;\n            goto nextname;\n        }\n\n        // error on unmatched '..', trying to go above root?\n        if (namelen == 2 && memcmp(name, \"..\", 2) == 0) {\n            return LFS_ERR_INVAL;\n        }\n\n        // skip if matched by '..' in name\n        const char *suffix = name + namelen;\n        lfs_size_t sufflen;\n        int depth = 1;\n        while (true) {\n            suffix += strspn(suffix, \"/\");\n            sufflen = strcspn(suffix, \"/\");\n            if (sufflen == 0) {\n                break;\n            }\n\n            if (sufflen == 1 && memcmp(suffix, \".\", 1) == 0) {\n                // noop\n            } else if (sufflen == 2 && memcmp(suffix, \"..\", 2) == 0) {\n                depth -= 1;\n                if (depth == 0) {\n                    name = suffix + sufflen;\n                    goto nextname;\n                }\n            } else {\n                depth += 1;\n            }\n\n            suffix += sufflen;\n        }\n\n        // found path\n        if (*name == '\\0') {\n            return tag;\n        }\n\n        // update what we've found so far\n        *path = name;\n\n        // only continue if we're a directory\n        if (lfs_tag_type3(tag) != LFS_TYPE_DIR) {\n            return LFS_ERR_NOTDIR;\n        }\n\n        // grab the entry data\n        if (lfs_tag_id(tag) != 0x3ff) {\n            lfs_stag_t res = lfs_dir_get(lfs, dir, LFS_MKTAG(0x700, 0x3ff, 0),\n                    LFS_MKTAG(LFS_TYPE_STRUCT, lfs_tag_id(tag), 8), dir->tail);\n            if (res < 0) {\n                return res;\n            }\n            lfs_pair_fromle32(dir->tail);\n        }\n\n        // find entry matching name\n        while (true) {\n            tag = lfs_dir_fetchmatch(lfs, dir, dir->tail,\n                    LFS_MKTAG(0x780, 0, 0),\n                    LFS_MKTAG(LFS_TYPE_NAME, 0, namelen),\n                    id,\n                    lfs_dir_find_match, &(struct lfs_dir_find_match){\n                        lfs, name, namelen});\n            if (tag < 0) {\n                return tag;\n            }\n\n            if (tag) {\n                break;\n            }\n\n            if (!dir->split) {\n                return LFS_ERR_NOENT;\n            }\n        }\n\n        // to next name\n        name += namelen;\n    }\n}\n...\nstatic inline bool lfs_path_islast(const char *path) {\n    lfs_size_t namelen = lfs_path_namelen(path);\n    return path[namelen + strspn(path + namelen, \"/\")] == '\\0';\n}\n...\nstatic inline lfs_size_t lfs_path_namelen(const char *path) {\n    return strcspn(path, \"/\");\n}\n...\nstatic void lfs_alloc_ckpoint(lfs_t *lfs) {\n    lfs->lookahead.ckpoint = lfs->block_count;\n}\n...\nstatic int lfs_dir_fetch(lfs_t *lfs,\n        lfs_mdir_t *dir, const lfs_block_t pair[2]) {\n    // note, mask=-1, tag=-1 can never match a tag since this\n    // pattern has the invalid bit set\n    return (int)lfs_dir_fetchmatch(lfs, dir, pair,\n            (lfs_tag_t)-1, (lfs_tag_t)-1, NULL, NULL, NULL);\n}\n...\nstatic inline void lfs_pair_tole32(lfs_block_t pair[2]) {\n    pair[0] = lfs_tole32(pair[0]);\n    pair[1] = lfs_tole32(pair[1]);\n}\n...\nstatic int lfs_dir_commit(lfs_t *lfs, lfs_mdir_t *dir,\n        const struct lfs_mattr *attrs, int attrcount) {\n    int orphans = lfs_dir_orphaningcommit(lfs, dir, attrs, attrcount);\n    if (orphans < 0) {\n        return orphans;\n    }\n\n    if (orphans) {\n        // make sure we've removed all orphans, this is a noop if there\n        // are none, but if we had nested blocks failures we may have\n        // created some\n        int err = lfs_fs_deorphan(lfs, false);\n        if (err) {\n            return err;\n        }\n    }\n\n    return 0;\n}\n...\nstatic inline void lfs_pair_fromle32(lfs_block_t pair[2]) {\n    pair[0] = lfs_fromle32(pair[0]);\n    pair[1] = lfs_fromle32(pair[1]);\n}\n...\nstatic int lfs_fs_preporphans(lfs_t *lfs, int8_t orphans) {\n    LFS_ASSERT(lfs_tag_size(lfs->gstate.tag) > 0x000 || orphans >= 0);\n    LFS_ASSERT(lfs_tag_size(lfs->gstate.tag) < 0x1ff || orphans <= 0);\n    lfs->gstate.tag += orphans;\n    lfs->gstate.tag = ((lfs->gstate.tag & ~LFS_MKTAG(0x800, 0, 0)) |\n            ((uint32_t)lfs_gstate_hasorphans(&lfs->gstate) << 31));\n\n    return 0;\n}\n...\nstatic int lfs_init(lfs_t *lfs, const struct lfs_config *cfg) {\n    lfs->cfg = cfg;\n    lfs->block_count = cfg->block_count;  // May be 0\n    int err = 0;\n\n#ifdef LFS_MULTIVERSION\n    // this driver only supports minor version < current minor version\n    LFS_ASSERT(!lfs->cfg->disk_version || (\n            (0xffff & (lfs->cfg->disk_version >> 16))\n                    == LFS_DISK_VERSION_MAJOR\n                && (0xffff & (lfs->cfg->disk_version >> 0))\n                    <= LFS_DISK_VERSION_MINOR));\n#endif\n\n    // check that bool is a truthy-preserving type\n    //\n    // note the most common reason for this failure is a before-c99 compiler,\n    // which littlefs currently does not support\n    LFS_ASSERT((bool)0x80000000);\n\n    // check that the required io functions are provided\n    LFS_ASSERT(lfs->cfg->read != NULL);\n#ifndef LFS_READONLY\n    LFS_ASSERT(lfs->cfg->prog != NULL);\n    LFS_ASSERT(lfs->cfg->erase != NULL);\n    LFS_ASSERT(lfs->cfg->sync != NULL);\n#endif\n\n    // validate that the lfs-cfg sizes were initiated properly before\n    // performing any arithmetic logics with them\n    LFS_ASSERT(lfs->cfg->read_size != 0);\n    LFS_ASSERT(lfs->cfg->prog_size != 0);\n    LFS_ASSERT(lfs->cfg->cache_size != 0);\n\n    // check that block size is a multiple of cache size is a multiple\n    // of prog and read sizes\n    LFS_ASSERT(lfs->cfg->cache_size % lfs->cfg->read_size == 0);\n    LFS_ASSERT(lfs->cfg->cache_size % lfs->cfg->prog_size == 0);\n    LFS_ASSERT(lfs->cfg->block_size % lfs->cfg->cache_size == 0);\n\n    // check that the block size is large enough to fit all ctz pointers\n    LFS_ASSERT(lfs->cfg->block_size >= 128);\n    // this is the exact calculation for all ctz pointers, if this fails\n    // and the simpler assert above does not, math must be broken\n    LFS_ASSERT(4*lfs_npw2(0xffffffff / (lfs->cfg->block_size-2*4))\n            <= lfs->cfg->block_size);\n\n    // block_cycles = 0 is no longer supported.\n    //\n    // block_cycles is the number of erase cycles before littlefs evicts\n    // metadata logs as a part of wear leveling. Suggested values are in the\n    // range of 100-1000, or set block_cycles to -1 to disable block-level\n    // wear-leveling.\n    LFS_ASSERT(lfs->cfg->block_cycles != 0);\n\n    // check that compact_thresh makes sense\n    //\n    // metadata can't be compacted below block_size/2, and metadata can't\n    // exceed a block_size\n    LFS_ASSERT(lfs->cfg->compact_thresh == 0\n            || lfs->cfg->compact_thresh >= lfs->cfg->block_size/2);\n    LFS_ASSERT(lfs->cfg->compact_thresh == (lfs_size_t)-1\n            || lfs->cfg->compact_thresh <= lfs->cfg->block_size);\n\n    // check that metadata_max is a multiple of read_size and prog_size,\n    // and a factor of the block_size\n    LFS_ASSERT(!lfs->cfg->metadata_max\n            || lfs->cfg->metadata_max % lfs->cfg->read_size == 0);\n    LFS_ASSERT(!lfs->cfg->metadata_max\n            || lfs->cfg->metadata_max % lfs->cfg->prog_size == 0);\n    LFS_ASSERT(!lfs->cfg->metadata_max\n            || lfs->cfg->block_size % lfs->cfg->metadata_max == 0);\n\n    // setup read cache\n    if (lfs->cfg->read_buffer) {\n        lfs->rcache.buffer = lfs->cfg->read_buffer;\n    } else {\n        lfs->rcache.buffer = lfs_malloc(lfs->cfg->cache_size);\n        if (!lfs->rcache.buffer) {\n            err = LFS_ERR_NOMEM;\n            goto cleanup;\n        }\n    }\n\n    // setup program cache\n    if (lfs->cfg->prog_buffer) {\n        lfs->pcache.buffer = lfs->cfg->prog_buffer;\n    } else {\n        lfs->pcache.buffer = lfs_malloc(lfs->cfg->cache_size);\n        if (!lfs->pcache.buffer) {\n            err = LFS_ERR_NOMEM;\n            goto cleanup;\n        }\n    }\n\n    // zero to avoid information leaks\n    lfs_cache_zero(lfs, &lfs->rcache);\n    lfs_cache_zero(lfs, &lfs->pcache);\n\n    // setup lookahead buffer, note mount finishes initializing this after\n    // we establish a decent pseudo-random seed\n    LFS_ASSERT(lfs->cfg->lookahead_size > 0);\n    if (lfs->cfg->lookahead_buffer) {\n        lfs->lookahead.buffer = lfs->cfg->lookahead_buffer;\n    } else {\n        lfs->lookahead.buffer = lfs_malloc(lfs->cfg->lookahead_size);\n        if (!lfs->lookahead.buffer) {\n            err = LFS_ERR_NOMEM;\n            goto cleanup;\n        }\n    }\n\n    // check that the size limits are sane\n    LFS_ASSERT(lfs->cfg->name_max <= LFS_NAME_MAX);\n    lfs->name_max = lfs->cfg->name_max;\n    if (!lfs->name_max) {\n        lfs->name_max = LFS_NAME_MAX;\n    }\n\n    LFS_ASSERT(lfs->cfg->file_max <= LFS_FILE_MAX);\n    lfs->file_max = lfs->cfg->file_max;\n    if (!lfs->file_max) {\n        lfs->file_max = LFS_FILE_MAX;\n    }\n\n    LFS_ASSERT(lfs->cfg->attr_max <= LFS_ATTR_MAX);\n    lfs->attr_max = lfs->cfg->attr_max;\n    if (!lfs->attr_max) {\n        lfs->attr_max = LFS_ATTR_MAX;\n    }\n\n    LFS_ASSERT(lfs->cfg->metadata_max <= lfs->cfg->block_size);\n\n    LFS_ASSERT(lfs->cfg->inline_max == (lfs_size_t)-1\n            || lfs->cfg->inline_max <= lfs->cfg->cache_size);\n    LFS_ASSERT(lfs->cfg->inline_max == (lfs_size_t)-1\n            || lfs->cfg->inline_max <= lfs->attr_max);\n    LFS_ASSERT(lfs->cfg->inline_max == (lfs_size_t)-1\n            || lfs->cfg->inline_max <= ((lfs->cfg->metadata_max)\n                ? lfs->cfg->metadata_max\n                : lfs->cfg->block_size)/8);\n    lfs->inline_max = lfs->cfg->inline_max;\n    if (lfs->inline_max == (lfs_size_t)-1) {\n        lfs->inline_max = 0;\n    } else if (lfs->inline_max == 0) {\n        lfs->inline_max = lfs_min(\n                lfs->cfg->cache_size,\n                lfs_min(\n                    lfs->attr_max,\n                    ((lfs->cfg->metadata_max)\n                        ? lfs->cfg->metadata_max\n                        : lfs->cfg->block_size)/8));\n    }\n\n    // setup default state\n    lfs->root[0] = LFS_BLOCK_NULL;\n    lfs->root[1] = LFS_BLOCK_NULL;\n    lfs->mlist = NULL;\n    lfs->seed = 0;\n    lfs->gdisk = (lfs_gstate_t){0};\n    lfs->gstate = (lfs_gstate_t){0};\n    lfs->gdelta = (lfs_gstate_t){0};\n#ifdef LFS_MIGRATE\n    lfs->lfs1 = NULL;\n#endif\n\n    return 0;\n\ncleanup:\n    lfs_deinit(lfs);\n    return err;\n}\n...\nstatic uint32_t lfs_fs_disk_version(lfs_t *lfs) {\n    (void)lfs;\n#ifdef LFS_MULTIVERSION\n    if (lfs->cfg->disk_version) {\n        return lfs->cfg->disk_version;\n    } else\n#endif\n    {\n        return LFS_DISK_VERSION;\n    }\n}\n...\nstatic inline void lfs_superblock_tole32(lfs_superblock_t *superblock) {\n    superblock->version     = lfs_tole32(superblock->version);\n    superblock->block_size  = lfs_tole32(superblock->block_size);\n    superblock->block_count = lfs_tole32(superblock->block_count);\n    superblock->name_max    = lfs_tole32(superblock->name_max);\n    superblock->file_max    = lfs_tole32(superblock->file_max);\n    superblock->attr_max    = lfs_tole32(superblock->attr_max);\n}\n...\nstatic int lfs_deinit(lfs_t *lfs) {\n    // free allocated memory\n    if (!lfs->cfg->read_buffer) {\n        lfs_free(lfs->rcache.buffer);\n    }\n\n    if (!lfs->cfg->prog_buffer) {\n        lfs_free(lfs->pcache.buffer);\n    }\n\n    if (!lfs->cfg->lookahead_buffer) {\n        lfs_free(lfs->lookahead.buffer);\n    }\n\n    return 0;\n}\n...\nstatic int lfs_ctz_extend(lfs_t *lfs,\n        lfs_cache_t *pcache, lfs_cache_t *rcache,\n        lfs_block_t head, lfs_size_t size,\n        lfs_block_t *block, lfs_off_t *off) {\n    while (true) {\n        // go ahead and grab a block\n        lfs_block_t nblock;\n        int err = lfs_alloc(lfs, &nblock);\n        if (err) {\n            return err;\n        }\n\n        {\n            err = lfs_bd_erase(lfs, nblock);\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n\n            if (size == 0) {\n                *block = nblock;\n                *off = 0;\n                return 0;\n            }\n\n            lfs_size_t noff = size - 1;\n            lfs_off_t index = lfs_ctz_index(lfs, &noff);\n            noff = noff + 1;\n\n            // just copy out the last block if it is incomplete\n            if (noff != lfs->cfg->block_size) {\n                for (lfs_off_t i = 0; i < noff; i++) {\n                    uint8_t data;\n                    err = lfs_bd_read(lfs,\n                            NULL, rcache, noff-i,\n                            head, i, &data, 1);\n                    if (err) {\n                        return err;\n                    }\n\n                    err = lfs_bd_prog(lfs,\n                            pcache, rcache, true,\n                            nblock, i, &data, 1);\n                    if (err) {\n                        if (err == LFS_ERR_CORRUPT) {\n                            goto relocate;\n                        }\n                        return err;\n                    }\n                }\n\n                *block = nblock;\n                *off = noff;\n                return 0;\n            }\n\n            // append block\n            index += 1;\n            lfs_size_t skips = lfs_ctz(index) + 1;\n            lfs_block_t nhead = head;\n            for (lfs_off_t i = 0; i < skips; i++) {\n                nhead = lfs_tole32(nhead);\n                err = lfs_bd_prog(lfs, pcache, rcache, true,\n                        nblock, 4*i, &nhead, 4);\n                nhead = lfs_fromle32(nhead);\n                if (err) {\n                    if (err == LFS_ERR_CORRUPT) {\n                        goto relocate;\n                    }\n                    return err;\n                }\n\n                if (i != skips-1) {\n                    err = lfs_bd_read(lfs,\n                            NULL, rcache, sizeof(nhead),\n                            nhead, 4*i, &nhead, sizeof(nhead));\n                    nhead = lfs_fromle32(nhead);\n                    if (err) {\n                        return err;\n                    }\n                }\n            }\n\n            *block = nblock;\n            *off = 4*skips;\n            return 0;\n        }\n\nrelocate:\n        LFS_DEBUG(\"Bad block at 0x%\"PRIx32, nblock);\n\n        // just clear cache and try a new block\n        lfs_cache_drop(lfs, pcache);\n    }\n}\n...\nstatic int lfs_file_relocate(lfs_t *lfs, lfs_file_t *file) {\n    while (true) {\n        // just relocate what exists into new block\n        lfs_block_t nblock;\n        int err = lfs_alloc(lfs, &nblock);\n        if (err) {\n            return err;\n        }\n\n        err = lfs_bd_erase(lfs, nblock);\n        if (err) {\n            if (err == LFS_ERR_CORRUPT) {\n                goto relocate;\n            }\n            return err;\n        }\n\n        // either read from dirty cache or disk\n        for (lfs_off_t i = 0; i < file->off; i++) {\n            uint8_t data;\n            if (file->flags & LFS_F_INLINE) {\n                err = lfs_dir_getread(lfs, &file->m,\n                        // note we evict inline files before they can be dirty\n                        NULL, &file->cache, file->off-i,\n                        LFS_MKTAG(0xfff, 0x1ff, 0),\n                        LFS_MKTAG(LFS_TYPE_INLINESTRUCT, file->id, 0),\n                        i, &data, 1);\n                if (err) {\n                    return err;\n                }\n            } else {\n                err = lfs_bd_read(lfs,\n                        &file->cache, &lfs->rcache, file->off-i,\n                        file->block, i, &data, 1);\n                if (err) {\n                    return err;\n                }\n            }\n\n            err = lfs_bd_prog(lfs,\n                    &lfs->pcache, &lfs->rcache, true,\n                    nblock, i, &data, 1);\n            if (err) {\n                if (err == LFS_ERR_CORRUPT) {\n                    goto relocate;\n                }\n                return err;\n            }\n        }\n\n        // copy over new state of file\n        memcpy(file->cache.buffer, lfs->pcache.buffer, lfs->cfg->cache_size);\n        file->cache.block = lfs->pcache.block;\n        file->cache.off = lfs->pcache.off;\n        file->cache.size = lfs->pcache.size;\n        lfs_cache_zero(lfs, &lfs->pcache);\n\n        file->block = nblock;\n        file->flags |= LFS_F_WRITING;\n        return 0;\n\nrelocate:\n        LFS_DEBUG(\"Bad block at 0x%\"PRIx32, nblock);\n\n        // just clear cache and try a new block\n        lfs_cache_drop(lfs, &lfs->pcache);\n    }\n}\n</additional code>\n<target_code>\nHere is the code fragment that you need to analyze:\n```c\nstatic int lfs_mkdir_(lfs_t *lfs, const char *path) {\n    // deorphan if we haven't yet, needed at most once after poweron\n    int err = lfs_fs_forceconsistency(lfs);\n    if (err) {\n        return err;\n    }\n\n    struct lfs_mlist cwd;\n    cwd.next = lfs->mlist;\n    uint16_t id;\n    err = lfs_dir_find(lfs, &cwd.m, &path, &id);\n    if (!(err == LFS_ERR_NOENT && lfs_path_islast(path))) {\n        return (err < 0) ? err : LFS_ERR_EXIST;\n    }\n...\n    // build up new directory\n    lfs_alloc_ckpoint(lfs);\n    lfs_mdir_t dir;\n    err = lfs_dir_alloc(lfs, &dir);\n    if (err) {\n        return err;\n    }\n...\n    return 0;\n}\n```\nFocus on the above code fragment and judge whether it violates the rule 17.2 or not.\n</target_code>\n",
  "is_rule_violated": false,
  "scenario": "No recursion detected. The lfs_mkdir_ fragment calls lfs_fs_forceconsistency, lfs_dir_find, lfs_alloc_ckpoint, and lfs_dir_alloc. lfs_dir_alloc calls allocation and block I/O helpers (lfs_alloc, lfs_bd_read, etc.) and utility functions (lfs_fromle32, lfs_alignup) but none of these call lfs_mkdir_. The call graph in the provided code does not contain any path from lfs_mkdir_ back to itself (directly or indirectly). Therefore Rule 17.2 is not violated."
}